{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb1d829e-c8fc-4edf-9c9e-0540c2c30a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb8c0ec2-342e-4c54-9a81-97642b2ef776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======\n",
      "\n",
      "\n",
      "Index(['poem name', 'content', 'author', 'type', 'age'], dtype='object')\n",
      "Shakespeare: 85 examples\n",
      "New Yorkers: 81 examples\n",
      "Shakespeare avg length: 1468.5058823529412\n",
      "New Yorkers avg length: 1810.6049382716049\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/shahules786/PoetryFoundationData/data/train-00000-of-00001-486832872ed96d17.parquet\")\n",
    "print(f\"\\n\\n======\\n\\n\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "newyork = df[df['author'].isin([\"John Ashbery\", \"Barbara Guest\", \"James Schuyler\", \"Kenneth Koch\", \"Frank O'Hara\"])]\n",
    "shake = df[df['author'] == 'William Shakespeare']\n",
    "\n",
    "print(f\"Shakespeare: {len(shake)} examples\\nNew Yorkers: {len(newyork)} examples\")\n",
    "print(f\"Shakespeare avg length: {np.average([len(poem) for poem in shake['content']])}\\nNew Yorkers avg length: {np.average([len(poem) for poem in newyork['content']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aecd7a4d-7185-4910-9292-e042a8859f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    }
   ],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each length 50\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-50\")\n",
    "    # wv_from_bin = api.load(\"glove.6B/glove.6B.50d.txt\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14ce4944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is a sentence', ' this is -another :sentence', ' and this is a question', ' again']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'sentence'],\n",
       " ['this', 'is', 'another', 'sentence'],\n",
       " ['and', 'this', 'is', 'a', 'question'],\n",
       " ['again']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_poem_debug(poem) :\n",
    "  out = re.sub(r'[\\r\\n]+', ' ', poem)\n",
    "  out = re.sub(r'[.?!]+', '.', out)\n",
    "  out = re.sub(r'\\s+', ' ', out)\n",
    "  out = out.lower()\n",
    "  sentence_list = out.split('.')\n",
    "  sentence_list = [sentence for sentence in sentence_list if len(sentence) > 0]\n",
    "  print(sentence_list)\n",
    "  for i in range(len(sentence_list)) :\n",
    "     sentence_list[i] = re.sub(r'[^a-zA-Z ]', '', sentence_list[i])\n",
    "  return [[word for word in sentence.split(' ') if word != ''] for sentence in sentence_list ]\n",
    "process_poem_debug(\"this is a sentence. This is -another :SENTENCE!!!!!\\nAND this is a question? again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e1031be-657a-4e09-86fb-5c592e3627f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of New Yorker sentences: 1412 with avg length of 17.378895184135978\n",
      "eg:\n",
      "   is anything central\n",
      "   orchards flung out on the land urban forests rustic plantations kneehigh hills\n",
      "   are place names central\n",
      "   elm grove adcock corner story book farm\n",
      "   as they concur with a rush at eye level beating themselves into eyes which have had enough thank you no more thank you\n",
      "   and they come on like scenery mingled with darkness the damp plains overgrown suburbs places of known civic pride of civil obscurity\n",
      "   these are connected to my version of america but the juice is elsewhere\n",
      "   this morning as i walked out of your room after breakfast crosshatched with backward and forward glances backward into light forward into unfamiliar light was it our doing and was it the material the lumber of life or of lives we were measuring counting\n",
      "   a mood soon to be forgotten in crossed girders of light cool downtown shadow in this morning that has seized us again\n",
      "   i know that i braid too much on my own snappedoff perceptions of things as they come to me\n",
      "\n",
      "Number of Shakespearean sentences: 811 with avg length of 25.398273736128235\n",
      "eg:\n",
      "   let the bird of loudest lay on the sole arabian tree herald sad and trumpet be to whose sound chaste wings obey\n",
      "   but thou shrieking harbinger foul precurrer of the fiend augur of the fevers end to this troop come thou not near\n",
      "   from this session interdict every fowl of tyrant wing save the eagle featherd king keep the obsequy so strict\n",
      "   let the priest in surplice white that defunctive music can be the deathdivining swan lest the requiem lack his right\n",
      "   and thou trebledated crow that thy sable gender makst with the breath thou givst and takst mongst our mourners shalt thou go\n",
      "   here the anthem doth commence love and constancy is dead phoenix and the turtle fled in a mutual flame from hence\n",
      "   so they lovd as love in twain had the essence but in one two distincts division none number there in love was slain\n",
      "   hearts remote yet not asunder distance and no space was seen twixt this turtle and his queen but in them it were a wonder\n",
      "   so between them love did shine that the turtle saw his right flaming in the phoenix sight either was the others mine\n",
      "   property was thus appalled that the self was not the same single natures double name neither two nor one was called\n",
      "\n",
      "\n",
      "Total vocab: 8505\n",
      "New York vocab: 5527\n",
      "Shakespeare Vocab: 4462\n",
      "Intersection: 1484\n",
      "New York vocab:\n",
      "   hoosiersto\n",
      "   turned\n",
      "   trumps\n",
      "   stretched\n",
      "   grabbing\n",
      "   mockery\n",
      "   replies\n",
      "   plot\n",
      "   crook\n",
      "   stew\n",
      "Shakespeare vocab:\n",
      "   twilight\n",
      "   esteems\n",
      "   inhabits\n",
      "   turned\n",
      "   son\n",
      "   wars\n",
      "   buriest\n",
      "   speechless\n",
      "   gazed\n",
      "   delay\n",
      "Both vocab:\n",
      "   twilight\n",
      "   turned\n",
      "   son\n",
      "   gazed\n",
      "   mockery\n",
      "   first\n",
      "   laughter\n",
      "   replies\n",
      "   enjoy\n",
      "   wood\n",
      "\n",
      "Total Bad Words (not in GloVe): 1133 out of total vocab 8505\n"
     ]
    }
   ],
   "source": [
    "def process_poem_into_list_of_words(poem) :\n",
    "  out = re.sub(r'[\\r\\n]+', ' ', poem)\n",
    "  out = re.sub(r'[.?!]+', '.', out)\n",
    "  out = re.sub(r'\\s+', ' ', out)\n",
    "  out = out.lower()\n",
    "  sentence_list = out.split('.')\n",
    "  sentence_list = [sentence for sentence in sentence_list if len(sentence) > 0]\n",
    "  for i in range(len(sentence_list)) :\n",
    "     sentence_list[i] = re.sub(r'[^a-zA-Z ]', '', sentence_list[i])\n",
    "  return [[word for word in sentence.split(' ') if word != ''] for sentence in sentence_list ]\n",
    "\n",
    "newyork_processed = [] # [process_poem_into_list_of_words(newyork['content'].iloc[i]) for i in range(len(newyork))]\n",
    "for i in range(len(newyork)) :\n",
    "   newyork_processed += process_poem_into_list_of_words(newyork['content'].iloc[i])\n",
    "newyork_labels = [0 for i in range(len(newyork_processed))]\n",
    "shake_processed = [] # [process_poem_into_list_of_words(shake['content'].iloc[i]) for i in range(len(shake))]\n",
    "for i in range(len(shake)) :\n",
    "   shake_processed += process_poem_into_list_of_words(shake['content'].iloc[i])\n",
    "shake_labels = [1 for i in range(len(shake_processed))]\n",
    "\n",
    "print(f\"Number of New Yorker sentences: {len(newyork_processed)} with avg length of {np.mean([len(sentence) for sentence in newyork_processed])}\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {' '.join(newyork_processed[i])}\")\n",
    "print(f\"\\nNumber of Shakespearean sentences: {len(shake_processed)} with avg length of {np.mean([len(sentence) for sentence in shake_processed])}\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {' '.join(shake_processed[i])}\")\n",
    "\n",
    "processed_poems = newyork_processed + shake_processed\n",
    "labels = newyork_labels + shake_labels\n",
    "# perm = np.random.permutation(len(processed_poems))\n",
    "# processed_poems = processed_poems[perm]\n",
    "# labels = labels[perm]\n",
    "newyork_vocab = set([word for poem in newyork_processed for word in poem])\n",
    "shake_vocab = set([word for poem in shake_processed for word in poem])\n",
    "vocab = sorted(list(set([word for poem in processed_poems for word in poem])))\n",
    "\n",
    "print(f\"\\n\\nTotal vocab: {len(vocab)}\\nNew York vocab: {len(newyork_vocab)}\\nShakespeare Vocab: {len(shake_vocab)}\\nIntersection: {len(shake_vocab & newyork_vocab)}\")\n",
    "print(f\"New York vocab:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {list(newyork_vocab)[i]}\")\n",
    "print(f\"Shakespeare vocab:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {list(shake_vocab)[i]}\")\n",
    "print(f\"Both vocab:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {list(newyork_vocab & shake_vocab)[i]}\")\n",
    "\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx:word for idx, word in enumerate(vocab)}\n",
    "\n",
    "for poem in processed_poems :\n",
    "  for i in range(len(poem)) :\n",
    "    poem[i] = word_to_idx[poem[i]]\n",
    "\n",
    "# print(processed_poems[0])\n",
    "\n",
    "embedding_matrix = np.zeros((len(vocab), 50))\n",
    "bad_count = 0\n",
    "for i, word in enumerate(vocab):\n",
    "    try:\n",
    "        embedding_matrix[i] = wv_from_bin.get_vector(word)\n",
    "    except:\n",
    "      #   print(\"this is bad\", word)\n",
    "        bad_count += 1\n",
    "print(f\"\\nTotal Bad Words (not in GloVe): {bad_count} out of total vocab {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92259646-dd57-4019-b319-613c9ca090b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length = 381\n",
      "Avg Length = 20.304543409806566\n",
      "Padded Poems shape is (2223, 50)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentence_lengths = [len(poem) for poem in processed_poems]\n",
    "max_length = max(sentence_lengths)\n",
    "avg_length = np.mean(sentence_lengths)\n",
    "print(f\"Max Length = {max_length}\\nAvg Length = {avg_length}\")\n",
    "max_length = 50\n",
    "padded_poems = pad_sequences(processed_poems, maxlen=max_length, padding='post', truncating='post')\n",
    "print(f\"Padded Poems shape is {np.array(padded_poems).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8b38356",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(padded_poems))\n",
    "shuffled_poems = np.array(padded_poems)[perm]\n",
    "shuffled_labels = np.array(labels)[perm]\n",
    "\n",
    "training_data = shuffled_poems[:-100]\n",
    "training_labels = shuffled_labels[:-100]\n",
    "\n",
    "validation_data = shuffled_poems[-100:]\n",
    "validation_labels = shuffled_labels[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc35f78",
   "metadata": {},
   "source": [
    "## LSTM Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7393db1a-470d-41cc-8db3-c437bb136103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │       \u001b[38;5;34m425,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "## hyperparams\n",
    "init_lr = 0.1\n",
    "lr_decay_rate = 0.5\n",
    "lr_decay_steps = 100\n",
    "dropout_p = 0.2\n",
    "l2_lambda = 0.005\n",
    "\n",
    "model_lstm = tf.keras.Sequential()\n",
    "e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "model_lstm.add(e)\n",
    "model_lstm.add(LSTM(100, input_shape = (max_length, 50)))\n",
    "model_lstm.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model_lstm.add(Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(init_lr, decay_rate=lr_decay_rate, decay_steps=lr_decay_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model_lstm.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c43a02a5-415d-42b1-bdd7-5db12108a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.6057 - loss: 2.9244\n",
      "Epoch 2/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.6330 - loss: 0.7540\n",
      "Epoch 3/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.6450 - loss: 0.6630\n",
      "Epoch 4/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6432 - loss: 0.6548\n",
      "Epoch 5/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6344 - loss: 0.6577\n",
      "Epoch 6/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.6171 - loss: 0.6674\n",
      "Epoch 7/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.6303 - loss: 0.6612\n",
      "Epoch 8/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.6481 - loss: 0.6494\n",
      "Epoch 9/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.6441 - loss: 0.6529\n",
      "Epoch 10/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.6281 - loss: 0.6603\n",
      "Epoch 11/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.6535 - loss: 0.6468\n",
      "Epoch 12/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.6289 - loss: 0.6602\n",
      "Epoch 13/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.6312 - loss: 0.6590\n",
      "Epoch 14/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.6361 - loss: 0.6555\n",
      "Epoch 15/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.6203 - loss: 0.6647\n",
      "Epoch 16/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6173 - loss: 0.6668\n",
      "Epoch 17/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.6350 - loss: 0.6565\n",
      "Epoch 18/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.6383 - loss: 0.6544\n",
      "Epoch 19/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.6333 - loss: 0.6576\n",
      "Epoch 20/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6437 - loss: 0.6518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x201365748c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(np.array(training_data), np.array(training_labels), epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97ec608f-e779-4a84-861f-9224bdfb21be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6690 - loss: 0.6389 \n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_lstm.evaluate(np.array(validation_data), np.array(validation_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7660ca1-7e6f-48dc-bd25-b5af1d0b80e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6384217739105225 0.6700000166893005\n"
     ]
    }
   ],
   "source": [
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111d1d5",
   "metadata": {},
   "source": [
    "## FCN - Logistic Regression Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e266e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │       \u001b[38;5;34m425,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout\n",
    "\n",
    "## hyperparams\n",
    "init_lr = 0.1\n",
    "lr_decay_rate = 0.5\n",
    "lr_decay_steps = 100\n",
    "dropout_p = 0.2\n",
    "l2_lambda = 0.005\n",
    "\n",
    "model_fcn = tf.keras.Sequential()\n",
    "e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "model_fcn.add(e)\n",
    "model_fcn.add(Flatten())\n",
    "model_fcn.add(Dropout(dropout_p))\n",
    "model_fcn.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(init_lr, decay_rate=lr_decay_rate, decay_steps=lr_decay_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model_fcn.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model_fcn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf498b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6193 - loss: 11.3169\n",
      "Epoch 2/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7773 - loss: 1.4741\n",
      "Epoch 3/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7519 - loss: 1.4653\n",
      "Epoch 4/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8123 - loss: 0.8336\n",
      "Epoch 5/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8282 - loss: 0.5814\n",
      "Epoch 6/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8613 - loss: 0.4724\n",
      "Epoch 7/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8413 - loss: 0.4785\n",
      "Epoch 8/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8741 - loss: 0.4126\n",
      "Epoch 9/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8558 - loss: 0.4454\n",
      "Epoch 10/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8571 - loss: 0.4100\n",
      "Epoch 11/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8679 - loss: 0.3947\n",
      "Epoch 12/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8700 - loss: 0.3912\n",
      "Epoch 13/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8692 - loss: 0.3991\n",
      "Epoch 14/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8673 - loss: 0.3856\n",
      "Epoch 15/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8622 - loss: 0.3957\n",
      "Epoch 16/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8712 - loss: 0.3807\n",
      "Epoch 17/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8612 - loss: 0.3924\n",
      "Epoch 18/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8644 - loss: 0.3904\n",
      "Epoch 19/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8863 - loss: 0.3798\n",
      "Epoch 20/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8823 - loss: 0.3800\n",
      "Epoch 21/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8876 - loss: 0.3650\n",
      "Epoch 22/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8734 - loss: 0.3888\n",
      "Epoch 23/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8724 - loss: 0.3755\n",
      "Epoch 24/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8661 - loss: 0.3993\n",
      "Epoch 25/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8742 - loss: 0.3736\n",
      "Epoch 26/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8638 - loss: 0.3820\n",
      "Epoch 27/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8640 - loss: 0.3847\n",
      "Epoch 28/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8655 - loss: 0.3848\n",
      "Epoch 29/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8623 - loss: 0.3934\n",
      "Epoch 30/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8686 - loss: 0.3770\n",
      "Epoch 31/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8701 - loss: 0.3755\n",
      "Epoch 32/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8768 - loss: 0.3808\n",
      "Epoch 33/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8876 - loss: 0.3576\n",
      "Epoch 34/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8646 - loss: 0.3887\n",
      "Epoch 35/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8719 - loss: 0.3873\n",
      "Epoch 36/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8888 - loss: 0.3803\n",
      "Epoch 37/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8785 - loss: 0.3848\n",
      "Epoch 38/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8675 - loss: 0.3856\n",
      "Epoch 39/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8796 - loss: 0.3799\n",
      "Epoch 40/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8831 - loss: 0.3622\n",
      "Epoch 41/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8682 - loss: 0.3833\n",
      "Epoch 42/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8629 - loss: 0.3952\n",
      "Epoch 43/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8828 - loss: 0.3738\n",
      "Epoch 44/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8733 - loss: 0.3840\n",
      "Epoch 45/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8704 - loss: 0.3834\n",
      "Epoch 46/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8734 - loss: 0.3883\n",
      "Epoch 47/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8691 - loss: 0.3976\n",
      "Epoch 48/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8756 - loss: 0.3763\n",
      "Epoch 49/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8706 - loss: 0.3834\n",
      "Epoch 50/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8681 - loss: 0.3764\n",
      "Epoch 51/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8825 - loss: 0.3740\n",
      "Epoch 52/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8688 - loss: 0.3809\n",
      "Epoch 53/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8798 - loss: 0.3783\n",
      "Epoch 54/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8564 - loss: 0.4060\n",
      "Epoch 55/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8941 - loss: 0.3598\n",
      "Epoch 56/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8863 - loss: 0.3555\n",
      "Epoch 57/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8765 - loss: 0.3712\n",
      "Epoch 58/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8668 - loss: 0.3944\n",
      "Epoch 59/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8678 - loss: 0.3828\n",
      "Epoch 60/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8764 - loss: 0.3796\n",
      "Epoch 61/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8779 - loss: 0.3733\n",
      "Epoch 62/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8804 - loss: 0.3778\n",
      "Epoch 63/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8892 - loss: 0.3613\n",
      "Epoch 64/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8601 - loss: 0.4046\n",
      "Epoch 65/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8844 - loss: 0.3645\n",
      "Epoch 66/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8790 - loss: 0.3714\n",
      "Epoch 67/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8760 - loss: 0.3759\n",
      "Epoch 68/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8818 - loss: 0.3737\n",
      "Epoch 69/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8798 - loss: 0.3903\n",
      "Epoch 70/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8742 - loss: 0.3766\n",
      "Epoch 71/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8752 - loss: 0.3833\n",
      "Epoch 72/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8625 - loss: 0.3825\n",
      "Epoch 73/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8699 - loss: 0.3999\n",
      "Epoch 74/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8583 - loss: 0.4075\n",
      "Epoch 75/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8685 - loss: 0.4069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x201363ea3c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fcn.fit(np.array(training_data), np.array(training_labels), epochs=75, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d51e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8865 - loss: 0.4345\n",
      "0.4613547623157501 0.8700000047683716\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_fcn.evaluate(np.array(validation_data), np.array(validation_labels), verbose=1)\n",
    "print(loss, accuracy)\n",
    "## best so far is about 85% at around 50 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adceaa",
   "metadata": {},
   "source": [
    "# Iterative Back Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b8c3f",
   "metadata": {},
   "source": [
    "First define the two models (S --> NY) and (NY --> S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3382a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anthony\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gpt2_tokenizer\\\\tokenizer_config.json',\n",
       " 'gpt2_tokenizer\\\\special_tokens_map.json',\n",
       " 'gpt2_tokenizer\\\\vocab.json',\n",
       " 'gpt2_tokenizer\\\\merges.txt',\n",
       " 'gpt2_tokenizer\\\\added_tokens.json',\n",
       " 'gpt2_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "# tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# tokenizer_gpt.save_pretrained('gpt2_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82dec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Flatten\n",
    "\n",
    "## hyperparams for both translation models\n",
    "dropout_p = 0.2\n",
    "l2_lambda = 0.005\n",
    "## end hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9303a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFT5ForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79550a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "S to NY model summary:\n",
      "=================\n",
      "NY to S model summary:\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2_tokenizer')\n",
    "\n",
    "model_s_to_ny = AutoModelForCausalLM.from_pretrained('fine_tuned_gpt2')\n",
    "print(\"=================\\nS to NY model summary:\")\n",
    "# print(model_s_to_ny.summary())\n",
    "\n",
    "model_ny_to_s = AutoModelForCausalLM.from_pretrained('fine_tuned_gpt2')\n",
    "print(\"=================\\nNY to S model summary:\")\n",
    "# print(model_ny_to_s.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caf1ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Shall I compare thee to a summer’s day? Thou art more lovely and more beautiful'},\n",
       " {'generated_text': 'This is another sentence, my dearest machine.\\r\\r\\n \\r\\r\\n \\r\\r'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "mainpipe = pipeline('text2text-generation', model=model_ny_to_s, tokenizer=tokenizer)\n",
    "code = [\"Shall I compare thee to a summer’s day? Thou art more lovely and more\", \"This is another sentence, my dearest machine.\"]\n",
    "mainpipe(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_s_to_ny = tf.keras.Sequential()\n",
    "# e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "# model_s_to_ny.add(e)\n",
    "# model_s_to_ny.add(LSTM(100, input_shape = (max_length, 50)))\n",
    "# model_s_to_ny.add(Dropout(dropout_p))\n",
    "# model_s_to_ny.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "# model_s_to_ny.add(Dropout(dropout_p))\n",
    "# model_s_to_ny.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "# model_s_to_ny.add(Dense(len(vocab), activation='softmax'))\n",
    "# print(model_s_to_ny.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41156321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ny_to_s = tf.keras.Sequential()\n",
    "# e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "# model_ny_to_s.add(e)\n",
    "# model_ny_to_s.add(LSTM(100, input_shape = (max_length, 50)))\n",
    "# model_ny_to_s.add(Dropout(dropout_p))\n",
    "# model_ny_to_s.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "# model_ny_to_s.add(Dropout(dropout_p))\n",
    "# model_ny_to_s.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "# model_ny_to_s.add(Dense(len(vocab), activation='softmax'))\n",
    "\n",
    "# print(model_ny_to_s.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e36327",
   "metadata": {},
   "source": [
    "Now define the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_discriminator = model_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141ea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_discriminator = tf.keras.Sequential()\n",
    "# e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "# model_discriminator.add(e)\n",
    "# model_discriminator.add(Flatten())\n",
    "# model_discriminator.add(Dropout(dropout_p))\n",
    "# model_discriminator.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b4e2d",
   "metadata": {},
   "source": [
    "Now, define optimizers for both the translation models and the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "659f6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "## disc opt hyperparam\n",
    "trans_init_lr = 0.1\n",
    "# disc_decay_rate = 0.5\n",
    "trans_decay_steps = 1000\n",
    "trans_decay_alpha = 0.0\n",
    "\n",
    "trans_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(trans_init_lr, trans_decay_steps, alpha=trans_decay_alpha)\n",
    "optimizer_trans = tf.keras.optimizers.Adam(learning_rate=trans_lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba722266",
   "metadata": {},
   "outputs": [],
   "source": [
    "## disc opt hyperparam\n",
    "disc_init_lr = 0.1\n",
    "# disc_decay_rate = 0.5\n",
    "disc_decay_steps = 1000\n",
    "disc_decay_alpha = 0.0\n",
    "\n",
    "disc_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(disc_init_lr, disc_decay_steps, alpha=disc_decay_alpha)\n",
    "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=disc_lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0064ebf",
   "metadata": {},
   "source": [
    "Pretrain the discriminator model for use in the IBT training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27762b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │       \u001b[38;5;34m425,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6254 - loss: 8.0983\n",
      "Epoch 2/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7109 - loss: 2.8355\n",
      "Epoch 3/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6791 - loss: 4.5700\n",
      "Epoch 4/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7619 - loss: 2.0303\n",
      "Epoch 5/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7692 - loss: 1.7464\n",
      "Epoch 6/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7659 - loss: 1.6173\n",
      "Epoch 7/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7253 - loss: 1.8871\n",
      "Epoch 8/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7550 - loss: 1.6226\n",
      "Epoch 9/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 1.1672\n",
      "Epoch 10/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7927 - loss: 0.9040\n",
      "Epoch 11/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8244 - loss: 0.7406\n",
      "Epoch 12/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8253 - loss: 0.5556\n",
      "Epoch 13/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8594 - loss: 0.4579\n",
      "Epoch 14/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8707 - loss: 0.4355\n",
      "Epoch 15/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8752 - loss: 0.3964\n",
      "Epoch 16/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8594 - loss: 0.4188\n",
      "Epoch 17/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8787 - loss: 0.3968\n",
      "Epoch 18/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8715 - loss: 0.4118\n",
      "Epoch 19/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8630 - loss: 0.4149\n",
      "Epoch 20/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8539 - loss: 0.4404\n",
      "Epoch 21/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8700 - loss: 0.3967\n",
      "Epoch 22/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8736 - loss: 0.4077\n",
      "Epoch 23/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8688 - loss: 0.4058\n",
      "Epoch 24/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8725 - loss: 0.3950\n",
      "Epoch 25/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8868 - loss: 0.3868\n",
      "Epoch 26/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8716 - loss: 0.4159\n",
      "Epoch 27/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8699 - loss: 0.4117\n",
      "Epoch 28/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8722 - loss: 0.4149\n",
      "Epoch 29/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8773 - loss: 0.4061\n",
      "Epoch 30/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8542 - loss: 0.4277\n",
      "Epoch 31/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.4190\n",
      "Epoch 32/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8735 - loss: 0.4173\n",
      "Epoch 33/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8593 - loss: 0.4165\n",
      "Epoch 34/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8695 - loss: 0.4123\n",
      "Epoch 35/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8599 - loss: 0.4245\n",
      "Epoch 36/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8700 - loss: 0.4160\n",
      "Epoch 37/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8767 - loss: 0.3934\n",
      "Epoch 38/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8710 - loss: 0.4177\n",
      "Epoch 39/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8654 - loss: 0.4167\n",
      "Epoch 40/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8817 - loss: 0.4039\n",
      "Epoch 41/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8715 - loss: 0.4103\n",
      "Epoch 42/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8768 - loss: 0.3996\n",
      "Epoch 43/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8732 - loss: 0.4315\n",
      "Epoch 44/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8622 - loss: 0.4269\n",
      "Epoch 45/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8679 - loss: 0.4176\n",
      "Epoch 46/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8687 - loss: 0.4291\n",
      "Epoch 47/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8807 - loss: 0.3969\n",
      "Epoch 48/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8752 - loss: 0.4097\n",
      "Epoch 49/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8675 - loss: 0.3977\n",
      "Epoch 50/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8663 - loss: 0.4184\n",
      "Epoch 51/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8772 - loss: 0.4182\n",
      "Epoch 52/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8533 - loss: 0.4131\n",
      "Epoch 53/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8758 - loss: 0.4192\n",
      "Epoch 54/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8695 - loss: 0.4093\n",
      "Epoch 55/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8889 - loss: 0.4026\n",
      "Epoch 56/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8521 - loss: 0.4438\n",
      "Epoch 57/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8623 - loss: 0.4178\n",
      "Epoch 58/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8648 - loss: 0.4149\n",
      "Epoch 59/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8595 - loss: 0.4344\n",
      "Epoch 60/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8551 - loss: 0.4517\n",
      "Epoch 61/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8782 - loss: 0.4161\n",
      "Epoch 62/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8753 - loss: 0.4150\n",
      "Epoch 63/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8753 - loss: 0.4141\n",
      "Epoch 64/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8673 - loss: 0.3997\n",
      "Epoch 65/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8615 - loss: 0.4206\n",
      "Epoch 66/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8692 - loss: 0.4187\n",
      "Epoch 67/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.3995\n",
      "Epoch 68/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8669 - loss: 0.4231\n",
      "Epoch 69/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8633 - loss: 0.4426\n",
      "Epoch 70/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8673 - loss: 0.4120\n",
      "Epoch 71/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8717 - loss: 0.4123\n",
      "Epoch 72/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8641 - loss: 0.4159\n",
      "Epoch 73/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8496 - loss: 0.4294\n",
      "Epoch 74/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8683 - loss: 0.4236\n",
      "Epoch 75/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8728 - loss: 0.4064\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8033 - loss: 0.5528\n",
      "0.5672714114189148 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "model_discriminator.compile(optimizer=optimizer_disc, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_discriminator.summary()\n",
    "model_discriminator.fit(np.array(training_data), np.array(training_labels), epochs=75, verbose=1)\n",
    "loss, accuracy = model_discriminator.evaluate(np.array(validation_data), np.array(validation_labels), verbose=1)\n",
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85af88f",
   "metadata": {},
   "source": [
    "Now, define the training loop of IBT (this is simplified from the paper, essentially just a GAN right now, it is not modeling a parallel corpus of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "892afcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Embedding name=embedding_5, built=True>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e32bd81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting...\n",
      "(20, 50) (40, 50)\n",
      "(20, 8505) (40, 8505)\n",
      "(20,) (40,)\n",
      "(20, 8505) (40, 8505)\n",
      "(20, 8505, 50) (40, 8505, 50)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs) : # num epochs\n",
    "    print(f\"Epoch {epoch + 1} starting...\")\n",
    "    curr_loss_trans = 0\n",
    "    curr_loss_disc = 0\n",
    "    for x_s_to_ny_b, x_ny_to_s_b in combined_ds :\n",
    "        print(x_s_to_ny_b.shape, x_ny_to_s_b.shape)\n",
    "        snyout = model_s_to_ny(x_s_to_ny_b)\n",
    "        nysout = model_ny_to_s(x_ny_to_s_b)\n",
    "        print(snyout.shape, nysout.shape)\n",
    "        snyarg = tf.argmax(snyout, axis=1)\n",
    "        nysarg = tf.argmax(nysout, axis=1)\n",
    "        print(snyarg.shape, nysarg.shape)\n",
    "        snyoh = tf.one_hot(snyarg, depth=len(vocab))\n",
    "        nysoh = tf.one_hot(nysarg, depth=len(vocab))\n",
    "        print(snyoh.shape, nysoh.shape)\n",
    "        snye = e(snyoh)\n",
    "        nyse = e(nysoh)\n",
    "        print(snye.shape, nyse.shape)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fa526ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_outputs(outputs) :\n",
    "    one_hots = tf.one_hot(tf.argmax(outputs, axis=1), depth=len(vocab))\n",
    "    embeddings = e(one_hots)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04cdc92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_s_to_ny, x_ny_to_s, recon_weight=1, adv_weight=1, verbose=False) :\n",
    "    with tf.GradientTape(persistent = True) as tape :\n",
    "        # forward pass\n",
    "        # s to ny forward and reconstruction\n",
    "        if verbose : print(f\"Starting Shakespeare to New Yorker forward pass\")\n",
    "        x_s_to_ny_out = embed_outputs(model_s_to_ny(x_s_to_ny))\n",
    "        x_s_to_ny_recon = embed_outputs(model_ny_to_s(x_s_to_ny_out))\n",
    "        x_s_to_ny_out_disc = model_discriminator(x_s_to_ny_out)\n",
    "        # ny to s forward and reconstruction\n",
    "        if verbose : print(f\"Starting New Yorker to Shakespeare forward pass\")\n",
    "        x_ny_to_s_out = embed_outputs(model_ny_to_s(x_ny_to_s))\n",
    "        x_ny_to_s_recon = embed_outputs(model_s_to_ny(x_ny_to_s_out))\n",
    "        x_ny_to_s_out_disc = model_discriminator(x_ny_to_s_out)\n",
    "\n",
    "        # losses\n",
    "        if verbose : print(f\"Starting loss calculations\")\n",
    "        # reconstruction loss\n",
    "        loss_s_to_ny_recon = tf.keras.losses.binary_crossentropy(x_s_to_ny, x_s_to_ny_recon)\n",
    "        loss_ny_to_s_recon = tf.keras.losses.binary_crossentropy(x_ny_to_s, x_ny_to_s_recon)\n",
    "        loss_recon = loss_s_to_ny_recon + loss_ny_to_s_recon\n",
    "        if verbose : print(f\"Calculated reconstruction loss as {loss_recon}\")\n",
    "        # adversarial loss\n",
    "        loss_s_to_ny_adv = tf.keras.losses.binary_crossentropy([0 for i in range(len(x_s_to_ny))], x_s_to_ny_out_disc)\n",
    "        loss_ny_to_s_adv = tf.keras.losses.binary_crossentropy([1 for i in range(len(x_ny_to_s))], x_ny_to_s_out_disc)\n",
    "        loss_adv = loss_s_to_ny_adv + loss_ny_to_s_adv\n",
    "        if verbose : print(f\"Calculated adversarial loss as {loss_adv}\")\n",
    "        # total loss for translators\n",
    "        loss_trans = loss_recon*recon_weight + loss_adv*adv_weight\n",
    "        if verbose : print(f\"Calculated total translation loss as {loss_trans}\")\n",
    "        # loss for discriminators\n",
    "        loss_disc = loss_adv\n",
    "        if verbose : print(f\"Calculated total loss for discriminator as {loss_disc}\")\n",
    "    \n",
    "    # gradients\n",
    "    if verbose : print(f\"Starting to apply gradients and update parameters for the translator models\")\n",
    "    gradients_trans = tape.gradient(loss_trans, model_s_to_ny.trainable_variables + model_ny_to_s.trainable_variables)\n",
    "    optimizer_trans.apply_gradients(zip(gradients_trans, model_s_to_ny.trainable_variables + model_ny_to_s.trainable_variables))\n",
    "    if verbose : print(f\"Finished updating the translator models\")\n",
    "\n",
    "    if verbose : print(f\"Starting to apply gradients and update parameters for the discriminator model\")\n",
    "    gradients_disc = tape.gradient(loss_disc, model_discriminator.trainable_variables)\n",
    "    optimizer_disc.apply_gradients(zip(gradients_disc, model_discriminator.trainable_variables))\n",
    "    if verbose : print(f\"Finished updating the discriminator model\")\n",
    "\n",
    "    if verbose : print(f\"Finishing training step.\\n\")\n",
    "    return loss_trans, loss_disc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8205a6c",
   "metadata": {},
   "source": [
    "Now, train over some batches for a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06226de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_s_to_ny shape is (711, 50)\n",
      "x_ny_to_s shape is (1412, 50)\n",
      "Batch size for s to ny: 20\n",
      "Number of batches: 36\n",
      "Batch size for ny to s: 40\n",
      "Number of batches: 36\n",
      "<_ZipDataset element_spec=(TensorSpec(shape=(None, 50), dtype=tf.int32, name=None), TensorSpec(shape=(None, 50), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "num_batches_to_make = 35\n",
    "\n",
    "raw_x_s_to_ny = [training_data[i] for i in range(len(training_labels)) if labels[i] == 1]\n",
    "raw_x_ny_to_s = [training_data[i] for i in range(len(training_labels)) if labels[i] == 0]\n",
    "\n",
    "print(f\"x_s_to_ny shape is {np.array(raw_x_s_to_ny).shape}\")\n",
    "print(f\"x_ny_to_s shape is {np.array(raw_x_ny_to_s).shape}\")\n",
    "\n",
    "ds_s_to_ny = tf.data.Dataset.from_tensor_slices(\n",
    "    raw_x_s_to_ny\n",
    ")\n",
    "ds_ny_to_s = tf.data.Dataset.from_tensor_slices(\n",
    "    raw_x_ny_to_s\n",
    ")\n",
    "\n",
    "ds_s_to_ny = ds_s_to_ny.batch(np.array(raw_x_s_to_ny).shape[0] // num_batches_to_make)\n",
    "ds_ny_to_s = ds_ny_to_s.batch(np.array(raw_x_ny_to_s).shape[0] // num_batches_to_make)\n",
    "\n",
    "for batch in ds_s_to_ny.take(1):  # Take the first batch\n",
    "    print(\"Batch size for s to ny:\", batch.shape[0])\n",
    "num_batches = tf.data.experimental.cardinality(ds_s_to_ny).numpy()\n",
    "print(\"Number of batches:\", num_batches)\n",
    "\n",
    "for batch in ds_ny_to_s.take(1):  # Take the first batch\n",
    "    print(\"Batch size for ny to s:\", batch.shape[0])\n",
    "num_batches = tf.data.experimental.cardinality(ds_ny_to_s).numpy()\n",
    "print(\"Number of batches:\", num_batches)\n",
    "\n",
    "combined_ds = tf.data.Dataset.zip((ds_s_to_ny, ds_ny_to_s))\n",
    "print(combined_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97f86d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting...\n",
      "Starting Shakespeare to New Yorker forward pass\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input [[[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n ...\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]]. Expected shape (20, 8505), but input has incompatible shape (20, 8505, 50)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(20, 8505, 50), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m curr_loss_disc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_s_to_ny_b, x_ny_to_s_b \u001b[38;5;129;01min\u001b[39;00m combined_ds :\n\u001b[1;32m---> 10\u001b[0m     loss_trans, loss_disc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_s_to_ny_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ny_to_s_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecon_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecon_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madv_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     curr_loss_trans \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_trans\n\u001b[0;32m     12\u001b[0m     curr_loss_disc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_disc\n",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(x_s_to_ny, x_ny_to_s, recon_weight, adv_weight, verbose)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose : \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Shakespeare to New Yorker forward pass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m x_s_to_ny_out \u001b[38;5;241m=\u001b[39m embed_outputs(model_s_to_ny(x_s_to_ny))\n\u001b[1;32m----> 7\u001b[0m x_s_to_ny_recon \u001b[38;5;241m=\u001b[39m embed_outputs(\u001b[43mmodel_ny_to_s\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_s_to_ny_out\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m x_s_to_ny_out_disc \u001b[38;5;241m=\u001b[39m model_discriminator(x_s_to_ny_out)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ny to s forward and reconstruction\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py:264\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input [[[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n ...\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]\n\n [[ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  ...\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]\n  [ 0.21705   0.46515  -0.46757  ... -0.043782  0.41013   0.1796  ]]]. Expected shape (20, 8505), but input has incompatible shape (20, 8505, 50)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(20, 8505, 50), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "recon_weight = 1\n",
    "adv_weight = 1\n",
    "##\n",
    "for epoch in range(num_epochs) : # num epochs\n",
    "    print(f\"Epoch {epoch + 1} starting...\")\n",
    "    curr_loss_trans = 0\n",
    "    curr_loss_disc = 0\n",
    "    for x_s_to_ny_b, x_ny_to_s_b in combined_ds :\n",
    "        loss_trans, loss_disc = train_step(x_s_to_ny_b, x_ny_to_s_b, recon_weight=recon_weight, adv_weight=adv_weight, verbose=True)\n",
    "        curr_loss_trans += loss_trans\n",
    "        curr_loss_disc += loss_disc\n",
    "        print(f\"   Batch Translator Loss: {loss_trans}\\n   Batch Discriminator Loss: {loss_disc}\")\n",
    "    print(f\"Total Epoch Translator Loss: {curr_loss_trans}\\nTotal Epoch Discriminator Loss: {curr_loss_disc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717bb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
