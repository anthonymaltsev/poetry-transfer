{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1d829e-c8fc-4edf-9c9e-0540c2c30a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb8c0ec2-342e-4c54-9a81-97642b2ef776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======\n",
      "\n",
      "\n",
      "Index(['poem name', 'content', 'author', 'type', 'age'], dtype='object')\n",
      "Shakespeare: 85 examples\n",
      "New Yorkers: 81 examples\n",
      "Shakespeare avg length: 1468.5058823529412\n",
      "New Yorkers avg length: 1810.6049382716049\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/shahules786/PoetryFoundationData/data/train-00000-of-00001-486832872ed96d17.parquet\")\n",
    "print(f\"\\n\\n======\\n\\n\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "newyork = df[df['author'].isin([\"John Ashbery\", \"Barbara Guest\", \"James Schuyler\", \"Kenneth Koch\", \"Frank O'Hara\"])]\n",
    "shake = df[df['author'] == 'William Shakespeare']\n",
    "\n",
    "print(f\"Shakespeare: {len(shake)} examples\\nNew Yorkers: {len(newyork)} examples\")\n",
    "print(f\"Shakespeare avg length: {np.average([len(poem) for poem in shake['content']])}\\nNew Yorkers avg length: {np.average([len(poem) for poem in newyork['content']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aecd7a4d-7185-4910-9292-e042a8859f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    }
   ],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each length 50\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-50\")\n",
    "    # wv_from_bin = api.load(\"glove.6B/glove.6B.50d.txt\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ce4944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is a sentence', ' this is -another :sentence', ' and this is a question', ' again']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'sentence'],\n",
       " ['this', 'is', 'another', 'sentence'],\n",
       " ['and', 'this', 'is', 'a', 'question'],\n",
       " ['again']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_poem_debug(poem) :\n",
    "  out = re.sub(r'[\\r\\n]+', ' ', poem)\n",
    "  out = re.sub(r'[.?!]+', '.', out)\n",
    "  out = re.sub(r'\\s+', ' ', out)\n",
    "  out = out.lower()\n",
    "  sentence_list = out.split('.')\n",
    "  sentence_list = [sentence for sentence in sentence_list if len(sentence) > 0]\n",
    "  print(sentence_list)\n",
    "  for i in range(len(sentence_list)) :\n",
    "     sentence_list[i] = re.sub(r'[^a-zA-Z ]', '', sentence_list[i])\n",
    "  return [[word for word in sentence.split(' ') if word != ''] for sentence in sentence_list ]\n",
    "process_poem_debug(\"this is a sentence. This is -another :SENTENCE!!!!!\\nAND this is a question? again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1031be-657a-4e09-86fb-5c592e3627f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of New Yorker sentences: 1412 with avg length of 17.378895184135978\n",
      "eg:\n",
      "   is anything central\n",
      "   orchards flung out on the land urban forests rustic plantations kneehigh hills\n",
      "   are place names central\n",
      "   elm grove adcock corner story book farm\n",
      "   as they concur with a rush at eye level beating themselves into eyes which have had enough thank you no more thank you\n",
      "   and they come on like scenery mingled with darkness the damp plains overgrown suburbs places of known civic pride of civil obscurity\n",
      "   these are connected to my version of america but the juice is elsewhere\n",
      "   this morning as i walked out of your room after breakfast crosshatched with backward and forward glances backward into light forward into unfamiliar light was it our doing and was it the material the lumber of life or of lives we were measuring counting\n",
      "   a mood soon to be forgotten in crossed girders of light cool downtown shadow in this morning that has seized us again\n",
      "   i know that i braid too much on my own snappedoff perceptions of things as they come to me\n",
      "\n",
      "Number of Shakespearean sentences: 811 with avg length of 25.398273736128235\n",
      "eg:\n",
      "   let the bird of loudest lay on the sole arabian tree herald sad and trumpet be to whose sound chaste wings obey\n",
      "   but thou shrieking harbinger foul precurrer of the fiend augur of the fevers end to this troop come thou not near\n",
      "   from this session interdict every fowl of tyrant wing save the eagle featherd king keep the obsequy so strict\n",
      "   let the priest in surplice white that defunctive music can be the deathdivining swan lest the requiem lack his right\n",
      "   and thou trebledated crow that thy sable gender makst with the breath thou givst and takst mongst our mourners shalt thou go\n",
      "   here the anthem doth commence love and constancy is dead phoenix and the turtle fled in a mutual flame from hence\n",
      "   so they lovd as love in twain had the essence but in one two distincts division none number there in love was slain\n",
      "   hearts remote yet not asunder distance and no space was seen twixt this turtle and his queen but in them it were a wonder\n",
      "   so between them love did shine that the turtle saw his right flaming in the phoenix sight either was the others mine\n",
      "   property was thus appalled that the self was not the same single natures double name neither two nor one was called\n",
      "\n",
      "\n",
      "Total vocab: 8505\n",
      "New York vocab: 5527\n",
      "Shakespeare Vocab: 4462\n",
      "Intersection: 1484\n",
      "New York vocab:\n",
      "   shiny\n",
      "   forks\n",
      "   singing\n",
      "   brilliant\n",
      "   plowing\n",
      "   couples\n",
      "   pogo\n",
      "   polish\n",
      "   candy\n",
      "   pertinent\n",
      "Shakespeare vocab:\n",
      "   elsewhere\n",
      "   statute\n",
      "   alteration\n",
      "   sunder\n",
      "   halfworld\n",
      "   sheep\n",
      "   mischief\n",
      "   ashypale\n",
      "   backwherein\n",
      "   firm\n",
      "Both vocab:\n",
      "   elsewhere\n",
      "   sheep\n",
      "   firm\n",
      "   teach\n",
      "   consider\n",
      "   appear\n",
      "   destroy\n",
      "   sunset\n",
      "   substance\n",
      "   haunted\n",
      "\n",
      "Total Bad Words (not in GloVe): 1133 out of total vocab 8505\n"
     ]
    }
   ],
   "source": [
    "def process_poem_into_list_of_words(poem) :\n",
    "  out = re.sub(r'[\\r\\n]+', ' ', poem)\n",
    "  out = re.sub(r'[.?!]+', '.', out)\n",
    "  out = re.sub(r'\\s+', ' ', out)\n",
    "  out = out.lower()\n",
    "  sentence_list = out.split('.')\n",
    "  sentence_list = [sentence for sentence in sentence_list if len(sentence) > 0]\n",
    "  for i in range(len(sentence_list)) :\n",
    "     sentence_list[i] = re.sub(r'[^a-zA-Z ]', '', sentence_list[i])\n",
    "  return [[word for word in sentence.split(' ') if word != ''] for sentence in sentence_list ]\n",
    "\n",
    "newyork_processed = [] # [process_poem_into_list_of_words(newyork['content'].iloc[i]) for i in range(len(newyork))]\n",
    "for i in range(len(newyork)) :\n",
    "   newyork_processed += process_poem_into_list_of_words(newyork['content'].iloc[i])\n",
    "newyork_labels = [0 for i in range(len(newyork_processed))]\n",
    "shake_processed = [] # [process_poem_into_list_of_words(shake['content'].iloc[i]) for i in range(len(shake))]\n",
    "for i in range(len(shake)) :\n",
    "   shake_processed += process_poem_into_list_of_words(shake['content'].iloc[i])\n",
    "shake_labels = [1 for i in range(len(shake_processed))]\n",
    "\n",
    "print(f\"Number of New Yorker sentences: {len(newyork_processed)} with avg length of {np.mean([len(sentence) for sentence in newyork_processed])}\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {' '.join(newyork_processed[i])}\")\n",
    "print(f\"\\nNumber of Shakespearean sentences: {len(shake_processed)} with avg length of {np.mean([len(sentence) for sentence in shake_processed])}\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {' '.join(shake_processed[i])}\")\n",
    "\n",
    "processed_poems = newyork_processed + shake_processed\n",
    "labels = newyork_labels + shake_labels\n",
    "# perm = np.random.permutation(len(processed_poems))\n",
    "# processed_poems = processed_poems[perm]\n",
    "# labels = labels[perm]\n",
    "newyork_vocab = set([word for poem in newyork_processed for word in poem])\n",
    "shake_vocab = set([word for poem in shake_processed for word in poem])\n",
    "vocab = sorted(list(set([word for poem in processed_poems for word in poem])))\n",
    "\n",
    "print(f\"\\n\\nTotal vocab: {len(vocab)}\\nNew York vocab: {len(newyork_vocab)}\\nShakespeare Vocab: {len(shake_vocab)}\\nIntersection: {len(shake_vocab & newyork_vocab)}\")\n",
    "print(f\"New York vocab:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {list(newyork_vocab)[i]}\")\n",
    "print(f\"Shakespeare vocab:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {list(shake_vocab)[i]}\")\n",
    "print(f\"Both vocab:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {list(newyork_vocab & shake_vocab)[i]}\")\n",
    "\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx:word for idx, word in enumerate(vocab)}\n",
    "\n",
    "for poem in processed_poems :\n",
    "  for i in range(len(poem)) :\n",
    "    poem[i] = word_to_idx[poem[i]]\n",
    "\n",
    "# print(processed_poems[0])\n",
    "\n",
    "embedding_matrix = np.zeros((len(vocab), 50))\n",
    "bad_count = 0\n",
    "for i, word in enumerate(vocab):\n",
    "    try:\n",
    "        embedding_matrix[i] = wv_from_bin.get_vector(word)\n",
    "    except:\n",
    "      #   print(\"this is bad\", word)\n",
    "        bad_count += 1\n",
    "print(f\"\\nTotal Bad Words (not in GloVe): {bad_count} out of total vocab {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92259646-dd57-4019-b319-613c9ca090b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length = 381\n",
      "Avg Length = 20.304543409806566\n",
      "Padded Poems shape is (2223, 50)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentence_lengths = [len(poem) for poem in processed_poems]\n",
    "max_length = max(sentence_lengths)\n",
    "avg_length = np.mean(sentence_lengths)\n",
    "print(f\"Max Length = {max_length}\\nAvg Length = {avg_length}\")\n",
    "max_length = 50\n",
    "padded_poems = pad_sequences(processed_poems, maxlen=max_length, padding='post', truncating='post')\n",
    "print(f\"Padded Poems shape is {np.array(padded_poems).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b38356",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(padded_poems))\n",
    "shuffled_poems = np.array(padded_poems)[perm]\n",
    "shuffled_labels = np.array(labels)[perm]\n",
    "\n",
    "training_data = shuffled_poems[:-100]\n",
    "training_labels = shuffled_labels[:-100]\n",
    "\n",
    "validation_data = shuffled_poems[-100:]\n",
    "validation_labels = shuffled_labels[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc35f78",
   "metadata": {},
   "source": [
    "## LSTM Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7393db1a-470d-41cc-8db3-c437bb136103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │       \u001b[38;5;34m425,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "## hyperparams\n",
    "init_lr = 0.1\n",
    "lr_decay_rate = 0.5\n",
    "lr_decay_steps = 100\n",
    "dropout_p = 0.2\n",
    "l2_lambda = 0.005\n",
    "\n",
    "model_lstm = tf.keras.Sequential()\n",
    "e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "model_lstm.add(e)\n",
    "model_lstm.add(LSTM(100, input_shape = (max_length, 50)))\n",
    "model_lstm.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model_lstm.add(Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(init_lr, decay_rate=lr_decay_rate, decay_steps=lr_decay_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model_lstm.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c43a02a5-415d-42b1-bdd7-5db12108a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 104ms/step - accuracy: 0.5934 - loss: 2.8671\n",
      "Epoch 2/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 104ms/step - accuracy: 0.6170 - loss: 0.7887\n",
      "Epoch 3/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 118ms/step - accuracy: 0.6372 - loss: 0.6772\n",
      "Epoch 4/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 115ms/step - accuracy: 0.6453 - loss: 0.6572\n",
      "Epoch 5/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 134ms/step - accuracy: 0.6223 - loss: 0.6658\n",
      "Epoch 6/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - accuracy: 0.6322 - loss: 0.6595\n",
      "Epoch 7/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 154ms/step - accuracy: 0.6283 - loss: 0.6611\n",
      "Epoch 8/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 156ms/step - accuracy: 0.6335 - loss: 0.6576\n",
      "Epoch 9/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 176ms/step - accuracy: 0.6392 - loss: 0.6549\n",
      "Epoch 10/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 173ms/step - accuracy: 0.6141 - loss: 0.6686\n",
      "Epoch 11/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 168ms/step - accuracy: 0.6407 - loss: 0.6532\n",
      "Epoch 12/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 173ms/step - accuracy: 0.6319 - loss: 0.6583\n",
      "Epoch 13/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 169ms/step - accuracy: 0.6380 - loss: 0.6549\n",
      "Epoch 14/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 205ms/step - accuracy: 0.6375 - loss: 0.6552\n",
      "Epoch 15/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step - accuracy: 0.6421 - loss: 0.6525\n",
      "Epoch 16/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step - accuracy: 0.6169 - loss: 0.6659\n",
      "Epoch 17/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 156ms/step - accuracy: 0.6322 - loss: 0.6581\n",
      "Epoch 18/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - accuracy: 0.6300 - loss: 0.6592\n",
      "Epoch 19/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 139ms/step - accuracy: 0.6356 - loss: 0.6561\n",
      "Epoch 20/20\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 148ms/step - accuracy: 0.6290 - loss: 0.6598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13aaaccebd0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(np.array(training_data), np.array(training_labels), epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "97ec608f-e779-4a84-861f-9224bdfb21be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6194 - loss: 0.6651\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_lstm.evaluate(np.array(validation_data), np.array(validation_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7660ca1-7e6f-48dc-bd25-b5af1d0b80e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6477655172348022 0.6499999761581421\n"
     ]
    }
   ],
   "source": [
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111d1d5",
   "metadata": {},
   "source": [
    "## FCN - Logistic Regression Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e266e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │       \u001b[38;5;34m425,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout\n",
    "\n",
    "## hyperparams\n",
    "init_lr = 0.1\n",
    "lr_decay_rate = 0.5\n",
    "lr_decay_steps = 100\n",
    "dropout_p = 0.2\n",
    "l2_lambda = 0.005\n",
    "\n",
    "model_fcn = tf.keras.Sequential()\n",
    "e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "model_fcn.add(e)\n",
    "model_fcn.add(Flatten())\n",
    "model_fcn.add(Dropout(dropout_p))\n",
    "model_fcn.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(init_lr, decay_rate=lr_decay_rate, decay_steps=lr_decay_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model_fcn.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model_fcn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf498b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5863 - loss: 66.4531\n",
      "Epoch 2/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6157 - loss: 25.3378\n",
      "Epoch 3/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7675 - loss: 5.5717\n",
      "Epoch 4/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7183 - loss: 4.2164\n",
      "Epoch 5/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7809 - loss: 2.7233\n",
      "Epoch 6/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7718 - loss: 2.3047\n",
      "Epoch 7/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8035 - loss: 2.0693\n",
      "Epoch 8/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8077 - loss: 1.8875\n",
      "Epoch 9/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8260 - loss: 1.6823\n",
      "Epoch 10/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8116 - loss: 1.7640\n",
      "Epoch 11/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8155 - loss: 1.5687\n",
      "Epoch 12/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8246 - loss: 1.4929\n",
      "Epoch 13/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7929 - loss: 1.6105\n",
      "Epoch 14/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8045 - loss: 1.5586\n",
      "Epoch 15/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8141 - loss: 1.5314\n",
      "Epoch 16/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8132 - loss: 1.5309\n",
      "Epoch 17/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8256 - loss: 1.4586\n",
      "Epoch 18/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8139 - loss: 1.5089\n",
      "Epoch 19/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8039 - loss: 1.5640\n",
      "Epoch 20/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8176 - loss: 1.4504\n",
      "Epoch 21/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8257 - loss: 1.4880\n",
      "Epoch 22/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8181 - loss: 1.5505\n",
      "Epoch 23/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8100 - loss: 1.5619\n",
      "Epoch 24/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8282 - loss: 1.4654\n",
      "Epoch 25/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7994 - loss: 1.5114\n",
      "Epoch 26/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8075 - loss: 1.5565\n",
      "Epoch 27/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8279 - loss: 1.4135\n",
      "Epoch 28/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8215 - loss: 1.4856\n",
      "Epoch 29/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8091 - loss: 1.6086\n",
      "Epoch 30/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8309 - loss: 1.4521\n",
      "Epoch 31/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8162 - loss: 1.4659\n",
      "Epoch 32/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8285 - loss: 1.5087\n",
      "Epoch 33/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8022 - loss: 1.5757\n",
      "Epoch 34/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8174 - loss: 1.5097\n",
      "Epoch 35/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8054 - loss: 1.5375\n",
      "Epoch 36/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8343 - loss: 1.4940\n",
      "Epoch 37/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8190 - loss: 1.4719\n",
      "Epoch 38/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8277 - loss: 1.4680\n",
      "Epoch 39/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8170 - loss: 1.4932\n",
      "Epoch 40/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8202 - loss: 1.4759\n",
      "Epoch 41/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8081 - loss: 1.4804\n",
      "Epoch 42/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8082 - loss: 1.5052\n",
      "Epoch 43/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8193 - loss: 1.5119\n",
      "Epoch 44/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8066 - loss: 1.5505\n",
      "Epoch 45/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8034 - loss: 1.5476\n",
      "Epoch 46/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8044 - loss: 1.5324\n",
      "Epoch 47/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8264 - loss: 1.4620\n",
      "Epoch 48/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8187 - loss: 1.4684\n",
      "Epoch 49/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8222 - loss: 1.5687\n",
      "Epoch 50/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8111 - loss: 1.5640\n",
      "Epoch 51/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8131 - loss: 1.4972\n",
      "Epoch 52/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8049 - loss: 1.5478\n",
      "Epoch 53/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8195 - loss: 1.4988\n",
      "Epoch 54/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8072 - loss: 1.5045\n",
      "Epoch 55/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8084 - loss: 1.5351\n",
      "Epoch 56/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8314 - loss: 1.4788\n",
      "Epoch 57/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8074 - loss: 1.5457\n",
      "Epoch 58/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8326 - loss: 1.4885\n",
      "Epoch 59/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8184 - loss: 1.5345\n",
      "Epoch 60/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8114 - loss: 1.5121\n",
      "Epoch 61/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8185 - loss: 1.5495\n",
      "Epoch 62/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8261 - loss: 1.4794\n",
      "Epoch 63/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8293 - loss: 1.4413\n",
      "Epoch 64/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8166 - loss: 1.5044\n",
      "Epoch 65/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8052 - loss: 1.5420\n",
      "Epoch 66/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8436 - loss: 1.4010\n",
      "Epoch 67/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8281 - loss: 1.4914\n",
      "Epoch 68/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8201 - loss: 1.5047\n",
      "Epoch 69/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8251 - loss: 1.4765\n",
      "Epoch 70/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8145 - loss: 1.5586\n",
      "Epoch 71/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7979 - loss: 1.5863\n",
      "Epoch 72/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8191 - loss: 1.4819\n",
      "Epoch 73/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7944 - loss: 1.6328\n",
      "Epoch 74/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8225 - loss: 1.5667\n",
      "Epoch 75/75\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8197 - loss: 1.5885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13ab3473a40>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fcn.fit(np.array(training_data), np.array(training_labels), epochs=75, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d51e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8018 - loss: 1.3027\n",
      "1.2511647939682007 0.8299999833106995\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_fcn.evaluate(np.array(validation_data), np.array(validation_labels), verbose=1)\n",
    "print(loss, accuracy)\n",
    "## best so far is about 85% at around 50 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adceaa",
   "metadata": {},
   "source": [
    "# Iterative Back Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b8c3f",
   "metadata": {},
   "source": [
    "First define the two models (S --> NY) and (NY --> S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82dec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Flatten\n",
    "\n",
    "## hyperparams for both translation models\n",
    "dropout_p = 0.2\n",
    "l2_lambda = 0.005\n",
    "## end hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c359fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │       \u001b[38;5;34m425,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_s_to_ny = tf.keras.Sequential()\n",
    "e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "model_s_to_ny.add(e)\n",
    "model_s_to_ny.add(LSTM(100, input_shape = (max_length, 50)))\n",
    "model_s_to_ny.add(Dropout(dropout_p))\n",
    "model_s_to_ny.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model_s_to_ny.add(Dropout(dropout_p))\n",
    "model_s_to_ny.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model_s_to_ny.add(Dense(len(vocab), activation='softmax'))\n",
    "print(model_s_to_ny.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41156321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │       \u001b[38;5;34m425,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,250</span> (1.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m425,250\u001b[0m (1.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_ny_to_s = tf.keras.Sequential()\n",
    "e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "model_ny_to_s.add(e)\n",
    "model_ny_to_s.add(LSTM(100, input_shape = (max_length, 50)))\n",
    "model_ny_to_s.add(Dropout(dropout_p))\n",
    "model_ny_to_s.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model_ny_to_s.add(Dropout(dropout_p))\n",
    "model_ny_to_s.add(Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model_ny_to_s.add(Dense(len(vocab), activation='softmax'))\n",
    "\n",
    "print(model_ny_to_s.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e36327",
   "metadata": {},
   "source": [
    "Now define the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e141ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_discriminator = tf.keras.Sequential()\n",
    "e = Embedding(len(vocab), 50, weights=[embedding_matrix], input_length = max_length, trainable=False)\n",
    "model_discriminator.add(e)\n",
    "model_discriminator.add(Flatten())\n",
    "model_discriminator.add(Dropout(dropout_p))\n",
    "model_discriminator.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b4e2d",
   "metadata": {},
   "source": [
    "Now, define optimizers for both the translation models and the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "659f6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "## disc opt hyperparam\n",
    "trans_init_lr = 0.1\n",
    "# disc_decay_rate = 0.5\n",
    "trans_decay_steps = 1000\n",
    "trans_decay_alpha = 0.0\n",
    "\n",
    "trans_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(trans_init_lr, trans_decay_steps, alpha=trans_decay_alpha)\n",
    "optimizer_trans = tf.keras.optimizers.Adam(learning_rate=trans_lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba722266",
   "metadata": {},
   "outputs": [],
   "source": [
    "## disc opt hyperparam\n",
    "disc_init_lr = 0.1\n",
    "# disc_decay_rate = 0.5\n",
    "disc_decay_steps = 1000\n",
    "disc_decay_alpha = 0.0\n",
    "\n",
    "disc_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(disc_init_lr, disc_decay_steps, alpha=disc_decay_alpha)\n",
    "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=disc_lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85af88f",
   "metadata": {},
   "source": [
    "Now, define the training loop of IBT (this is simplified from the paper, essentially just a GAN right now, it is not modeling a parallel corpus of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04cdc92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_s_to_ny, x_ny_to_s, recon_weight=1, adv_weight=1, verbose=False) :\n",
    "    with tf.GradientTape(persistent = True) as tape :\n",
    "        # forward pass\n",
    "        # s to ny forward and reconstruction\n",
    "        if verbose : print(f\"Starting Shakespeare to New Yorker forward pass\")\n",
    "        x_s_to_ny_out = model_s_to_ny(x_s_to_ny)\n",
    "        x_s_to_ny_recon = model_ny_to_s(x_s_to_ny_out)\n",
    "        x_s_to_ny_out_disc = model_discriminator(x_s_to_ny_out)\n",
    "        # ny to s forward and reconstruction\n",
    "        if verbose : print(f\"Starting New Yorker to Shakespear forward pass\")\n",
    "        x_ny_to_s_out = model_ny_to_s(x_ny_to_s)\n",
    "        x_ny_to_s_recon = model_s_to_ny(x_ny_to_s_out)\n",
    "        x_ny_to_s_out_disc = model_discriminator(x_ny_to_s_out)\n",
    "\n",
    "        # losses\n",
    "        if verbose : print(f\"Starting loss calculations\")\n",
    "        # reconstruction loss\n",
    "        loss_s_to_ny_recon = tf.keras.losses.binary_crossentropy(x_s_to_ny, x_s_to_ny_recon)\n",
    "        loss_ny_to_s_recon = tf.keras.losses.binary_crossentropy(x_ny_to_s, x_ny_to_s_recon)\n",
    "        loss_recon = loss_s_to_ny_recon + loss_ny_to_s_recon\n",
    "        if verbose : print(f\"Calculated reconstruction loss as {loss_recon}\")\n",
    "        # adversarial loss\n",
    "        loss_s_to_ny_adv = tf.keras.losses.binary_crossentropy([0 for i in range(len(x_s_to_ny))], x_s_to_ny_out_disc)\n",
    "        loss_ny_to_s_adv = tf.keras.losses.binary_crossentropy([1 for i in range(len(x_ny_to_s))], x_ny_to_s_out_disc)\n",
    "        loss_adv = loss_s_to_ny_adv + loss_ny_to_s_adv\n",
    "        if verbose : print(f\"Calculated adversarial loss as {loss_adv}\")\n",
    "        # total loss for translators\n",
    "        loss_trans = loss_recon*recon_weight + loss_adv*adv_weight\n",
    "        if verbose : print(f\"Calculated total translation loss as {loss_trans}\")\n",
    "        # loss for discriminators\n",
    "        loss_disc = loss_adv\n",
    "        if verbose : print(f\"Calculated total loss for discriminator as {loss_disc}\")\n",
    "    \n",
    "    # gradients\n",
    "    if verbose : print(f\"Starting to apply gradients and update parameters for the translator models\")\n",
    "    gradients_trans = tape.gradient(loss_trans, model_s_to_ny.trainable_variables + model_ny_to_s.trainable_variables)\n",
    "    optimizer_trans.apply_gradients(zip(gradients_trans, model_s_to_ny.trainable_variables + model_ny_to_s.trainable_variables))\n",
    "    if verbose : print(f\"Finished updating the translator models\")\n",
    "\n",
    "    if verbose : print(f\"Starting to apply gradients and update parameters for the discriminator model\")\n",
    "    gradients_disc = tape.gradient(loss_disc, model_discriminator.trainable_variables)\n",
    "    optimizer_disc.apply_gradients(zip(gradients_disc, model_discriminator.trainable_variables))\n",
    "    if verbose : print(f\"Finished updating the discriminator model\")\n",
    "\n",
    "    if verbose : print(f\"Finishing training step.\\n\")\n",
    "    return loss_trans, loss_disc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8205a6c",
   "metadata": {},
   "source": [
    "Now, train over some batches for a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea14974e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 4\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100).batch(32)\n",
    "\n",
    "# Get the number of batches\n",
    "num_batches = tf.data.experimental.cardinality(dataset).numpy()\n",
    "print(\"Number of batches:\", num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06226de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_s_to_ny shape is (711, 50)\n",
      "x_ny_to_s shape is (1412, 50)\n",
      "Batch size for s to ny: 35\n",
      "Number of batches: 21\n",
      "Batch size for ny to s: 70\n",
      "Number of batches: 21\n",
      "<_ZipDataset element_spec=(TensorSpec(shape=(None, 50), dtype=tf.int32, name=None), TensorSpec(shape=(None, 50), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "raw_x_s_to_ny = [training_data[i] for i in range(len(training_labels)) if labels[i] == 1]\n",
    "raw_x_ny_to_s = [training_data[i] for i in range(len(training_labels)) if labels[i] == 0]\n",
    "\n",
    "print(f\"x_s_to_ny shape is {np.array(raw_x_s_to_ny).shape}\")\n",
    "print(f\"x_ny_to_s shape is {np.array(raw_x_ny_to_s).shape}\")\n",
    "\n",
    "ds_s_to_ny = tf.data.Dataset.from_tensor_slices(\n",
    "    raw_x_s_to_ny\n",
    ")\n",
    "ds_ny_to_s = tf.data.Dataset.from_tensor_slices(\n",
    "    raw_x_ny_to_s\n",
    ")\n",
    "\n",
    "ds_s_to_ny = ds_s_to_ny.batch(np.array(raw_x_s_to_ny).shape[0] // 20)\n",
    "ds_ny_to_s = ds_ny_to_s.batch(np.array(raw_x_ny_to_s).shape[0] // 20)\n",
    "\n",
    "for batch in ds_s_to_ny.take(1):  # Take the first batch\n",
    "    print(\"Batch size for s to ny:\", batch.shape[0])\n",
    "num_batches = tf.data.experimental.cardinality(ds_s_to_ny).numpy()\n",
    "print(\"Number of batches:\", num_batches)\n",
    "\n",
    "for batch in ds_ny_to_s.take(1):  # Take the first batch\n",
    "    print(\"Batch size for ny to s:\", batch.shape[0])\n",
    "num_batches = tf.data.experimental.cardinality(ds_ny_to_s).numpy()\n",
    "print(\"Number of batches:\", num_batches)\n",
    "\n",
    "combined_ds = tf.data.Dataset.zip((ds_s_to_ny, ds_ny_to_s))\n",
    "print(combined_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97f86d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting...\n",
      "Starting Shakespeare to New Yorker forward pass\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(10) : # num epochs\n",
    "    print(f\"Epoch {epoch + 1} starting...\")\n",
    "    curr_loss_trans = 0\n",
    "    curr_loss_disc = 0\n",
    "    for x_s_to_ny, x_ny_to_s in combined_ds :\n",
    "        loss_trans, loss_disc = train_step(training_data, x_ny_to_s, recon_weight=1, adv_weight=1, verbose=True)\n",
    "        curr_loss_trans += loss_trans\n",
    "        curr_loss_disc += loss_disc\n",
    "        print(f\"   Batch Translator Loss: {loss_trans}\\n   Batch Discriminator Loss: {loss_disc}\")\n",
    "    print(f\"Total Epoch Translator Loss: {curr_loss_trans}\\nTotal Epoch Discriminator Loss: {curr_loss_disc}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
