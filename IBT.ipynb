{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe columns: Index(['poem name', 'content', 'author', 'type', 'age'], dtype='object')\n",
      "Shakespeare: 85 examples\n",
      "New Yorkers: 81 examples\n",
      "Shakespeare avg length: 1468.5058823529412\n",
      "New Yorkers avg length: 1810.6049382716049\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/shahules786/PoetryFoundationData/data/train-00000-of-00001-486832872ed96d17.parquet\")\n",
    "\n",
    "print(f\"dataframe columns: {df.columns}\")\n",
    "\n",
    "newyork = df[df['author'].isin([\"John Ashbery\", \"Barbara Guest\", \"James Schuyler\", \"Kenneth Koch\", \"Frank O'Hara\"])]\n",
    "shake = df[df['author'] == 'William Shakespeare']\n",
    "\n",
    "print(f\"Shakespeare: {len(shake)} examples\\nNew Yorkers: {len(newyork)} examples\")\n",
    "print(f\"Shakespeare avg length: {np.average([len(poem) for poem in shake['content']])}\\nNew Yorkers avg length: {np.average([len(poem) for poem in newyork['content']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a sentence.',\n",
       " 'This is -another :SENTENCE!!!!!',\n",
       " 'AND this is a question?',\n",
       " 'again.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_poem(poem) :\n",
    "  proc = re.sub(r'[\\r\\n]+', ' ', poem)\n",
    "  proc = re.sub(r'\\s+', ' ', proc)\n",
    "  sentences = re.split(r'(?<=[.!?])\\s+', proc)\n",
    "  sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "  return sentences\n",
    "process_poem(\"this is a sentence. This is -another :SENTENCE!!!!!\\nAND this is a question? again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of New Yorker sentences: 1213 with avg length of 110.57378400659522 characters\n",
      "eg:\n",
      "    Is anything central?\n",
      "   Orchards flung out on the land, Urban forests, rustic plantations, knee-high hills?\n",
      "   Are place names central?\n",
      "   Elm Grove, Adcock Corner, Story Book Farm?\n",
      "   As they concur with a rush at eye level Beating themselves into eyes which have had enough Thank you, no more thank you.\n",
      "   And they come on like scenery mingled with darkness The damp plains, overgrown suburbs, Places of known civic pride, of civil obscurity.\n",
      "   These are connected to my version of America But the juice is elsewhere.\n",
      "   This morning as I walked out of your room After breakfast crosshatched with Backward and forward glances, backward into light, Forward into unfamiliar light, Was it our doing, and was it The material, the lumber of life, or of lives We were measuring, counting?\n",
      "   A mood soon to be forgotten In crossed girders of light, cool downtown shadow In this morning that has seized us again?\n",
      "   I know that I braid too much on my own Snapped-off perceptions of things as they come to me.\n",
      "\n",
      "Number of Shakespearean sentences: 645 with avg length of 174.8372093023256 characters\n",
      "eg:\n",
      "    Let the bird of loudest lay On the sole Arabian tree Herald sad and trumpet be, To whose sound chaste wings obey.\n",
      "   But thou shrieking harbinger, Foul precurrer of the fiend, Augur of the fever's end, To this troop come thou not near.\n",
      "   From this session interdict Every fowl of tyrant wing, Save the eagle, feather'd king; Keep the obsequy so strict.\n",
      "   Let the priest in surplice white, That defunctive music can, Be the death-divining swan, Lest the requiem lack his right.\n",
      "   And thou treble-dated crow, That thy sable gender mak'st With the breath thou giv'st and tak'st, 'Mongst our mourners shalt thou go.\n",
      "   Here the anthem doth commence: Love and constancy is dead; Phoenix and the Turtle fled In a mutual flame from hence.\n",
      "   So they lov'd, as love in twain Had the essence but in one; Two distincts, division none: Number there in love was slain.\n",
      "   Hearts remote, yet not asunder; Distance and no space was seen 'Twixt this Turtle and his queen: But in them it were a wonder.\n",
      "   So between them love did shine That the Turtle saw his right Flaming in the Phoenix' sight: Either was the other's mine.\n",
      "   Property was thus appalled That the self was not the same; Single nature's double name Neither two nor one was called.\n"
     ]
    }
   ],
   "source": [
    "newyork_processed = [] \n",
    "for i in range(len(newyork)) :\n",
    "   newyork_processed += process_poem(newyork['content'].iloc[i])\n",
    "newyork_labels = [0 for i in range(len(newyork_processed))]\n",
    "\n",
    "shake_processed = [] \n",
    "for i in range(len(shake)) :\n",
    "   shake_processed += process_poem(shake['content'].iloc[i])\n",
    "shake_labels = [1 for i in range(len(shake_processed))]\n",
    "\n",
    "processed_poems = newyork_processed + shake_processed\n",
    "labels = newyork_labels + shake_labels\n",
    "\n",
    "##\n",
    "\n",
    "print(f\"Number of New Yorker sentences: {len(newyork_processed)} with avg length of {np.mean([len(sentence) for sentence in newyork_processed])} characters\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {newyork_processed[i]}\")\n",
    "print(f\"\\nNumber of Shakespearean sentences: {len(shake_processed)} with avg length of {np.mean([len(sentence) for sentence in shake_processed])} characters\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {shake_processed[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length = 2707\n",
      "Avg Length = 132.8826695371367\n"
     ]
    }
   ],
   "source": [
    "sentence_lengths = [len(poem) for poem in processed_poems]\n",
    "max_length = max(sentence_lengths)\n",
    "avg_length = np.mean(sentence_lengths)\n",
    "print(f\"Max Length = {max_length}\\nAvg Length = {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(processed_poems))\n",
    "shuffled_poems = np.array(processed_poems)[perm]\n",
    "shuffled_labels = np.array(labels)[perm]\n",
    "\n",
    "training_data = shuffled_poems[:-100]\n",
    "training_labels = shuffled_labels[:-100]\n",
    "\n",
    "validation_data = shuffled_poems[-100:]\n",
    "validation_labels = shuffled_labels[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Back Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initialize the two models (S --> NY) and (NY --> S) to pretrained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Flatten\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM, TFAutoModelForSequenceClassification, TFT5ForConditionalGeneration, pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFGPT2ForSequenceClassification.\n",
      "\n",
      "All the weights of TFGPT2ForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2ForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load initial translator models\n",
    "tokenizer_trans = AutoTokenizer.from_pretrained('gemini_tokenizer')\n",
    "tokenizer_trans.pad_token = tokenizer_trans.eos_token\n",
    "model_s_to_ny = TFT5ForConditionalGeneration.from_pretrained('fine_tuned_gemini')\n",
    "model_s_to_ny.config.pad_token_id = model_s_to_ny.config.eos_token_id\n",
    "model_ny_to_s = TFT5ForConditionalGeneration.from_pretrained('fine_tuned_gemini')\n",
    "model_ny_to_s.config.pad_token_id = model_ny_to_s.config.eos_token_id\n",
    "\n",
    "# load discriminator model\n",
    "model_disc = TFAutoModelForSequenceClassification.from_pretrained('gpt2_discriminator')\n",
    "model_disc.config.pad_token_id = model_disc.config.eos_token_id\n",
    "tokenizer_disc = AutoTokenizer.from_pretrained('gpt2_discriminator_tokenizer')\n",
    "tokenizer_disc.pad_token = tokenizer_disc.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=================\\nS to NY model summary:\")\n",
    "print(model_s_to_ny.summary())\n",
    "print(\"\\n\\n=================\\nNY to S model summary:\")\n",
    "print(model_ny_to_s.summary())\n",
    "print(\"\\n\\n=================\\nDiscriminator model summary:\")\n",
    "print(model_disc.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out initial models on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shall I compare thee to a summer’s day? Thou art more lovely', 'This is another sentence, my dearest machine.']\n"
     ]
    }
   ],
   "source": [
    "testpipe = pipeline('text2text-generation', model=model_ny_to_s, tokenizer=tokenizer_trans)\n",
    "prompt = [\"Shall I compare thee to a summer’s day? Thou art more lovely and more\", \"This is another sentence, my dearest machine.\"]\n",
    "out = [model_output['generated_text'] for model_output in testpipe(prompt)]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-4.6278176  5.755144 ]\n",
      " [ 3.7093012 -6.571079 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor([1 0], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Shall I compare thee to a summer’s day? Thou art more lovely and more\", \"I look at you and I would rather look at you than all the portraits in the world\"]\n",
    "# prompts = \"I look at you and I would rather look at you than all the portraits in the world\"\n",
    "inputs = tokenizer_disc(prompts, return_tensors='tf', padding=True, truncation=True)\n",
    "logits = model_disc(**inputs).logits\n",
    "print(logits)\n",
    "predicted_class_id = tf.math.argmax(logits, axis=-1)\n",
    "print(predicted_class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define optimizers for both the translation models and the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## disc opt hyperparam\n",
    "trans_init_lr = 0.1\n",
    "# disc_decay_rate = 0.5\n",
    "trans_decay_steps = 1000\n",
    "trans_decay_alpha = 0.0\n",
    "\n",
    "trans_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(trans_init_lr, trans_decay_steps, alpha=trans_decay_alpha)\n",
    "optimizer_trans = tf.keras.optimizers.Adam(learning_rate=trans_lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## disc opt hyperparam\n",
    "disc_init_lr = 0.1\n",
    "# disc_decay_rate = 0.5\n",
    "disc_decay_steps = 1000\n",
    "disc_decay_alpha = 0.0\n",
    "\n",
    "disc_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(disc_init_lr, disc_decay_steps, alpha=disc_decay_alpha)\n",
    "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=disc_lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the training loop of IBT (this is simplified from the paper, essentially just a GAN right now, it is not modeling a parallel corpus of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model pipelines\n",
    "internal_pipe_s_to_ny = pipeline(\"text2text-generation\", model=model_s_to_ny, tokenizer=tokenizer_trans)\n",
    "def pipe_s_to_ny(prompts) :\n",
    "    return [out['generated_text'] for out in internal_pipe_s_to_ny(prompts)]\n",
    "def pipe_s_to_ny_tensor(prompts) :\n",
    "    inputs = [s.decode('utf-8') for s in prompts.numpy().tolist()]\n",
    "    return [out['generated_text'] for out in internal_pipe_s_to_ny(inputs)]\n",
    "\n",
    "internal_pipe_ny_to_s = pipeline(\"text2text-generation\", model=model_ny_to_s, tokenizer=tokenizer_trans)\n",
    "def pipe_ny_to_s(prompts) :\n",
    "    return [out['generated_text'] for out in internal_pipe_ny_to_s(prompts)]\n",
    "def pipe_ny_to_s_tensor(prompts) :\n",
    "    inputs = [s.decode('utf-8') for s in prompts.numpy().tolist()]\n",
    "    return [out['generated_text'] for out in internal_pipe_ny_to_s(inputs)]\n",
    "\n",
    "# custom classification pipeline\n",
    "def pipe_disc(prompts, from_logits=False) :\n",
    "    inputs = tokenizer_disc(prompts, return_tensors='tf', padding=True, truncation=True)\n",
    "    logits = model_disc(**inputs).logits\n",
    "    if from_logits :\n",
    "        predicted_class_id = tf.math.reduce_max(logits, axis=-1)\n",
    "    else :\n",
    "        predicted_class_id = tf.math.argmax(logits, axis=-1) # 1 is shake, 0 is ny\n",
    "    return predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define function that will construct a mini dataset for the translators to train on\n",
    "def construct_parallel_corpus_tf(inputs, targets) :\n",
    "    model_inputs = tokenizer_trans(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    with tokenizer_trans.as_target_tokenizer() :\n",
    "        model_targets = tokenizer_trans(\n",
    "            targets,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "    ins = model_inputs['input_ids']\n",
    "    att = model_inputs['attention_mask']\n",
    "    lab = model_targets['input_ids']\n",
    "    lab = tf.where(lab == tokenizer_trans.pad_token_id, -100, lab)\n",
    "    return ins, att, lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_s_to_ny, x_ny_to_s, train_weight=1, adv_weight=1, verbose=False) :\n",
    "    with tf.GradientTape() as tape_s_to_ny, tf.GradientTape() as tape_ny_to_s :\n",
    "        tape_s_to_ny.watch(model_s_to_ny.trainable_variables)\n",
    "        tape_ny_to_s.watch(model_ny_to_s.trainable_variables)\n",
    "        # forward pass\n",
    "        # s to ny forward\n",
    "        if verbose : print(f\"Starting Shakespeare to New Yorker forward pass\")\n",
    "        x_s_to_ny_out = pipe_s_to_ny(x_s_to_ny)\n",
    "        x_s_to_ny_out_disc = pipe_disc(x_s_to_ny_out, from_logits=True)\n",
    "        # ny to s forward\n",
    "        if verbose : print(f\"Starting New Yorker to Shakespeare forward pass\")\n",
    "        x_ny_to_s_out = pipe_ny_to_s(x_ny_to_s)\n",
    "        x_ny_to_s_out_disc = pipe_disc(x_ny_to_s_out, from_logits=True)\n",
    "\n",
    "        # compute pseudo parallel corpora to train on\n",
    "        if verbose : print(f\"Constructing Parallel Corpus for Shakespeare to NY\")\n",
    "        inputs_ids_s_to_ny, attention_mask_s_to_ny, labels_s_to_ny = construct_parallel_corpus_tf(x_ny_to_s_out, x_ny_to_s)\n",
    "        if verbose : print(f\"Constructing Parallel Corpus for NY to Shakespeare\")\n",
    "        inputs_ids_ny_to_s, attention_mask_ny_to_s, labels_ny_to_s = construct_parallel_corpus_tf(x_s_to_ny_out, x_s_to_ny)\n",
    "\n",
    "        # train on pseudo parallel corpus data\n",
    "        if verbose : print(f\"Training S to NY model on pseudo parallel corpus\")\n",
    "        trainout_s_to_ny = model_s_to_ny(\n",
    "            input_ids = inputs_ids_s_to_ny,\n",
    "            attention_mask = attention_mask_s_to_ny,\n",
    "            labels = labels_s_to_ny,\n",
    "            training=True\n",
    "        )\n",
    "        if verbose : print(f\"Training NY to S model on pseudo parallel corpus\")\n",
    "        trainout_ny_to_s = model_ny_to_s(\n",
    "            input_ids = inputs_ids_ny_to_s,\n",
    "            attention_mask = attention_mask_ny_to_s,\n",
    "            labels = labels_ny_to_s,\n",
    "            training=True\n",
    "        )\n",
    "\n",
    "        # losses\n",
    "        if verbose : print(f\"Starting loss calculations for both models training phases\")\n",
    "        loss_s_to_ny_train = trainout_s_to_ny.loss\n",
    "        loss_ny_to_s_train = trainout_ny_to_s.loss\n",
    "        if verbose : print(f\"Starting additional discriminator loss calculations\")\n",
    "        loss_s_to_ny_adv = tf.keras.losses.binary_crossentropy([0 for i in range(len(x_s_to_ny))], x_s_to_ny_out_disc, from_logits=True)\n",
    "        loss_ny_to_s_adv = tf.keras.losses.binary_crossentropy([1 for i in range(len(x_ny_to_s))], x_ny_to_s_out_disc, from_logits=True)\n",
    "        # total losses\n",
    "        loss_s_to_ny = train_weight * loss_s_to_ny_train + adv_weight * loss_s_to_ny_adv\n",
    "        loss_ny_to_s = train_weight * loss_ny_to_s_train + adv_weight * loss_ny_to_s_adv\n",
    "        if verbose : print(f\"Calculated S to NY loss as {round(loss_s_to_ny, 2)} (train loss = {round(loss_s_to_ny_train, 2)}, adv loss = {round(loss_s_to_ny_adv, 2)})\")\n",
    "        if verbose : print(f\"Calculated NY to S loss as {round(loss_ny_to_s, 2)} (train loss = {round(loss_ny_to_s_train, 2)}, adv loss = {round(loss_ny_to_s_adv, 2)})\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    # gradients\n",
    "    if verbose : print(f\"Starting to apply gradients and update parameters for the translator models\")\n",
    "    gradients_s_to_ny = tape_s_to_ny.gradient(loss_s_to_ny, model_s_to_ny.trainable_variables)\n",
    "    optimizer_trans.apply_gradients(zip(gradients_s_to_ny, model_s_to_ny.trainable_variables))\n",
    "\n",
    "    gradients_ny_to_s = tape_ny_to_s.gradient(loss_ny_to_s, model_ny_to_s.trainable_variables)\n",
    "    optimizer_trans.apply_gradients(zip(gradients_ny_to_s, model_ny_to_s.trainable_variables))\n",
    "    if verbose : print(f\"Finished updating the translator models\")\n",
    "\n",
    "    return loss_s_to_ny, loss_ny_to_s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train over some batches for a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "raw_x_s_to_ny = [training_data[i] for i in range(len(training_labels)) if labels[i] == 1]\n",
    "sny_cardinality = len(raw_x_s_to_ny)\n",
    "raw_x_s_to_ny = raw_x_s_to_ny + raw_x_s_to_ny[:batch_size]\n",
    "raw_x_ny_to_s = [training_data[i] for i in range(len(training_labels)) if labels[i] == 0]\n",
    "nys_cardinality = len(raw_x_ny_to_s)\n",
    "raw_x_ny_to_s = raw_x_ny_to_s + raw_x_ny_to_s[:batch_size]\n",
    "\n",
    "\n",
    "def get_training_batch(batch_num, batch_size=8) :\n",
    "    curr_sny_idx = (batch_num * batch_size) % sny_cardinality\n",
    "    curr_nys_idx = (batch_num * batch_size) % nys_cardinality\n",
    "    x_s_to_ny = raw_x_s_to_ny[curr_sny_idx:curr_sny_idx+batch_size]\n",
    "    x_ny_to_s = raw_x_ny_to_s[curr_nys_idx:curr_nys_idx+batch_size]\n",
    "    return x_s_to_ny, x_ny_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "num_batches_per_epoch = max(nys_cardinality//batch_size, sny_cardinality//batch_size)\n",
    "print(num_batches_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting...\n",
      "Starting Shakespeare to New Yorker forward pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "train_weight = 1\n",
    "adv_weight = 1\n",
    "##\n",
    "for epoch in range(num_epochs) : # num epochs\n",
    "    print(f\"Epoch {epoch + 1} starting...\")\n",
    "    epoch_loss_sny = 0\n",
    "    epoch_loss_nys = 0\n",
    "    for batch_num in range(num_batches_per_epoch) :\n",
    "        x_s_to_ny, x_ny_to_s = get_training_batch(batch_num)\n",
    "        loss_sny, loss_nys = train_step(x_s_to_ny, x_ny_to_s, train_weight=train_weight, adv_weight=adv_weight, verbose=True)\n",
    "        epoch_loss_sny += loss_sny\n",
    "        epoch_loss_nys += loss_nys\n",
    "        print(f\"   Batch S to NY Loss: {loss_sny}\\n   Batch NY to S Loss: {loss_nys}\")\n",
    "    print(f\"Total Epoch S to NY Loss: {epoch_loss_sny}\\nTotal Epoch NY to S Loss: {epoch_loss_nys}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
