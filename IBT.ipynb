{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBpASLiIOSlt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeD-ZfxiJE_p",
    "outputId": "4bfce4fd-34ef-4c8e-c800-dc1753eeb222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17Ei5odIHHVK"
   },
   "source": [
    "# Main Iterative Back Translation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_mEOGGDG_6Q"
   },
   "source": [
    "## Create Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtDJp2hIG7uo",
    "outputId": "a75e5010-0469-4ddb-955f-462656f77d0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe columns: Index(['poem name', 'content', 'author', 'type', 'age'], dtype='object')\n",
      "Shakespeare: 85 examples\n",
      "New Yorkers: 81 examples\n",
      "Shakespeare avg length: 1468.5058823529412\n",
      "New Yorkers avg length: 1810.6049382716049\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "df = pd.read_parquet(\"hf://datasets/shahules786/PoetryFoundationData/data/train-00000-of-00001-486832872ed96d17.parquet\")\n",
    "\n",
    "print(f\"dataframe columns: {df.columns}\")\n",
    "\n",
    "newyork = df[df['author'].isin([\"John Ashbery\", \"Barbara Guest\", \"James Schuyler\", \"Kenneth Koch\", \"Frank O'Hara\"])]\n",
    "shake = df[df['author'] == 'William Shakespeare']\n",
    "\n",
    "print(f\"Shakespeare: {len(shake)} examples\\nNew Yorkers: {len(newyork)} examples\")\n",
    "print(f\"Shakespeare avg length: {np.average([len(poem) for poem in shake['content']])}\\nNew Yorkers avg length: {np.average([len(poem) for poem in newyork['content']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESVhMUTPG5Tz",
    "outputId": "4c3c9bd3-eb86-44b4-a660-b70e1e2e9b0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a sentence.',\n",
       " 'This is -another :SENTENCE!!!!!',\n",
       " 'AND this is a question?',\n",
       " 'again.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_poem(poem) :\n",
    "  proc = re.sub(r'[\\r\\n]+', ' ', poem)\n",
    "  proc = re.sub(r'\\s+', ' ', proc)\n",
    "  sentences = re.split(r'(?<=[.!?])\\s+', proc)\n",
    "  sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "  return sentences\n",
    "process_poem(\"this is a sentence. This is -another :SENTENCE!!!!!\\nAND this is a question? again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TFor9J0Exkk",
    "outputId": "23d0c2cf-c22e-4f4b-b7c7-c383070cdd77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of New Yorker sentences: 1213 with avg length of 110.57378400659522 characters\n",
      "eg:\n",
      "    Is anything central?\n",
      "   Orchards flung out on the land, Urban forests, rustic plantations, knee-high hills?\n",
      "   Are place names central?\n",
      "   Elm Grove, Adcock Corner, Story Book Farm?\n",
      "   As they concur with a rush at eye level Beating themselves into eyes which have had enough Thank you, no more thank you.\n",
      "   And they come on like scenery mingled with darkness The damp plains, overgrown suburbs, Places of known civic pride, of civil obscurity.\n",
      "   These are connected to my version of America But the juice is elsewhere.\n",
      "   This morning as I walked out of your room After breakfast crosshatched with Backward and forward glances, backward into light, Forward into unfamiliar light, Was it our doing, and was it The material, the lumber of life, or of lives We were measuring, counting?\n",
      "   A mood soon to be forgotten In crossed girders of light, cool downtown shadow In this morning that has seized us again?\n",
      "   I know that I braid too much on my own Snapped-off perceptions of things as they come to me.\n",
      "\n",
      "Number of Shakespearean sentences: 645 with avg length of 174.8372093023256 characters\n",
      "eg:\n",
      "    Let the bird of loudest lay On the sole Arabian tree Herald sad and trumpet be, To whose sound chaste wings obey.\n",
      "   But thou shrieking harbinger, Foul precurrer of the fiend, Augur of the fever's end, To this troop come thou not near.\n",
      "   From this session interdict Every fowl of tyrant wing, Save the eagle, feather'd king; Keep the obsequy so strict.\n",
      "   Let the priest in surplice white, That defunctive music can, Be the death-divining swan, Lest the requiem lack his right.\n",
      "   And thou treble-dated crow, That thy sable gender mak'st With the breath thou giv'st and tak'st, 'Mongst our mourners shalt thou go.\n",
      "   Here the anthem doth commence: Love and constancy is dead; Phoenix and the Turtle fled In a mutual flame from hence.\n",
      "   So they lov'd, as love in twain Had the essence but in one; Two distincts, division none: Number there in love was slain.\n",
      "   Hearts remote, yet not asunder; Distance and no space was seen 'Twixt this Turtle and his queen: But in them it were a wonder.\n",
      "   So between them love did shine That the Turtle saw his right Flaming in the Phoenix' sight: Either was the other's mine.\n",
      "   Property was thus appalled That the self was not the same; Single nature's double name Neither two nor one was called.\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets for training\n",
    "\n",
    "\n",
    "newyork_processed = []\n",
    "for i in range(len(newyork)) :\n",
    "   newyork_processed += process_poem(newyork['content'].iloc[i])\n",
    "newyork_labels = [0 for i in range(len(newyork_processed))]\n",
    "\n",
    "shake_processed = []\n",
    "for i in range(len(shake)) :\n",
    "   shake_processed += process_poem(shake['content'].iloc[i])\n",
    "shake_labels = [1 for i in range(len(shake_processed))]\n",
    "\n",
    "processed_poems = newyork_processed + shake_processed\n",
    "labels = newyork_labels + shake_labels\n",
    "\n",
    "##\n",
    "\n",
    "print(f\"Number of New Yorker sentences: {len(newyork_processed)} with avg length of {np.mean([len(sentence) for sentence in newyork_processed])} characters\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {newyork_processed[i]}\")\n",
    "print(f\"\\nNumber of Shakespearean sentences: {len(shake_processed)} with avg length of {np.mean([len(sentence) for sentence in shake_processed])} characters\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {shake_processed[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmtAnwvlGzAy",
    "outputId": "2ee5b4a0-c389-42a9-e88f-c0f111f1df11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length = 2707\n",
      "Avg Length = 132.8826695371367\n"
     ]
    }
   ],
   "source": [
    "sentence_lengths = [len(poem) for poem in processed_poems]\n",
    "max_length = max(sentence_lengths)\n",
    "avg_length = np.mean(sentence_lengths)\n",
    "print(f\"Max Length = {max_length}\\nAvg Length = {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j4W7YsxQGwqr"
   },
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(processed_poems))\n",
    "shuffled_poems = np.array(processed_poems)[perm]\n",
    "shuffled_labels = np.array(labels)[perm]\n",
    "\n",
    "training_data = shuffled_poems[:-100]\n",
    "training_labels = shuffled_labels[:-100]\n",
    "\n",
    "validation_data = shuffled_poems[-100:]\n",
    "validation_labels = shuffled_labels[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMXNzFmMGm6g"
   },
   "source": [
    "## Iterative Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "62TH-X_iGhqD"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Flatten\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM, TFAutoModelForSequenceClassification, TFT5ForConditionalGeneration, pipeline, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e_O8MHXaRwl",
    "outputId": "615cd65f-4289-4c38-cc3c-35cdbb234e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUS: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Setting Memory Growth limits for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f\"GPUS: {gpus}\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(f\"Setting Memory Growth limits for {gpu}\")\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6nUHKzyGjw2"
   },
   "source": [
    "First initialize the two models (S --> NY) and (NY --> S) to pretrained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mrnCBS6QGgLx",
    "outputId": "e484e4ed-9fcf-456b-db05-da0e90b94e50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFGPT2ForSequenceClassification.\n",
      "\n",
      "All the weights of TFGPT2ForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2ForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load initial translator models\n",
    "tokenizer_trans = AutoTokenizer.from_pretrained('/content/drive/MyDrive/cs230projmodels/gemini_small_tokenizer')\n",
    "tokenizer_trans.pad_token = tokenizer_trans.eos_token\n",
    "model_s_to_ny = TFT5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/cs230projmodels/fine_tuned_gemini_small')\n",
    "model_s_to_ny.config.pad_token_id = model_s_to_ny.config.eos_token_id\n",
    "model_ny_to_s = TFT5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/cs230projmodels/fine_tuned_gemini_small')\n",
    "model_ny_to_s.config.pad_token_id = model_ny_to_s.config.eos_token_id\n",
    "\n",
    "# load discriminator model\n",
    "model_disc = TFAutoModelForSequenceClassification.from_pretrained('/content/drive/MyDrive/cs230projmodels/gpt2_discriminator')\n",
    "model_disc.config.pad_token_id = model_disc.config.eos_token_id\n",
    "tokenizer_disc = AutoTokenizer.from_pretrained('/content/drive/MyDrive/cs230projmodels/gpt2_discriminator_tokenizer')\n",
    "tokenizer_disc.pad_token = tokenizer_disc.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OXRTyzqGdMR",
    "outputId": "7b7374e8-c257-4407-da6d-d0d06983155d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "S to NY model summary:\n",
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  24674304  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  109628544 \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  137949312 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222903552 (850.31 MB)\n",
      "Trainable params: 222903552 (850.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=================\n",
      "NY to S model summary:\n",
      "Model: \"tft5_for_conditional_generation_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  24674304  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  109628544 \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  137949312 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222903552 (850.31 MB)\n",
      "Trainable params: 222903552 (850.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=================\n",
      "Discriminator model summary:\n",
      "Model: \"tfgpt2_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " score (Dense)               multiple                  1536      \n",
      "                                                                 \n",
      " transformer (TFGPT2MainLay  multiple                  124439808 \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124441344 (474.71 MB)\n",
      "Trainable params: 124441344 (474.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"=================\\nS to NY model summary:\")\n",
    "print(model_s_to_ny.summary())\n",
    "print(\"\\n\\n=================\\nNY to S model summary:\")\n",
    "print(model_ny_to_s.summary())\n",
    "print(\"\\n\\n=================\\nDiscriminator model summary:\")\n",
    "print(model_disc.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI2D_g7pGSXP"
   },
   "source": [
    "Test out initial models on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QOX545TjGPch"
   },
   "outputs": [],
   "source": [
    "# testpipe = pipeline('text2text-generation', model=model_ny_to_s, tokenizer=tokenizer_trans, device=0)\n",
    "# prompt = [\"Shall I compare thee to a summer’s day? Thou art more lovely and more\", \"This is another sentence, my dearest machine.\"]\n",
    "# out = [model_output['generated_text'] for model_output in testpipe(prompt)]\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NfxQYKNJGMPE"
   },
   "outputs": [],
   "source": [
    "# prompts = [\"Shall I compare thee to a summer’s day? Thou art more lovely and more\", \"I look at you and I would rather look at you than all the portraits in the world\"]\n",
    "# inputs = tokenizer_disc(prompts, return_tensors='tf', padding=True, truncation=True)\n",
    "# logits = model_disc(**inputs).logits\n",
    "# print(logits)\n",
    "# predicted_class_id = tf.math.argmax(logits, axis=-1)\n",
    "# print(predicted_class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uS2MQOyLF_K3"
   },
   "source": [
    "Now, define optimizers for the translators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SuYy4OVuGDKM"
   },
   "outputs": [],
   "source": [
    "trans_init_lr = 0.1\n",
    "trans_decay_steps = 1000\n",
    "trans_decay_alpha = 0.0\n",
    "\n",
    "# trans_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(trans_init_lr, trans_decay_steps, alpha=trans_decay_alpha)\n",
    "optimizer_s_to_ny = tf.keras.optimizers.Adam(learning_rate=trans_init_lr) #trans_lr_schedule)\n",
    "optimizer_ny_to_s = tf.keras.optimizers.Adam(learning_rate=trans_init_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOEVH-J2FwCr"
   },
   "source": [
    "Now, define the training loop of IBT including an adversarial discriminator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_HZU0xeRFunr"
   },
   "outputs": [],
   "source": [
    "# define model pipelines\n",
    "internal_pipe_s_to_ny = pipeline(\"text2text-generation\", model=model_s_to_ny, tokenizer=tokenizer_trans, device=0)\n",
    "def pipe_s_to_ny(prompts) :\n",
    "    return [out['generated_text'] for out in internal_pipe_s_to_ny(prompts)]\n",
    "def pipe_s_to_ny_tensor(prompts) :\n",
    "    inputs = [s.decode('utf-8') for s in prompts.numpy().tolist()]\n",
    "    return [out['generated_text'] for out in internal_pipe_s_to_ny(inputs)]\n",
    "\n",
    "internal_pipe_ny_to_s = pipeline(\"text2text-generation\", model=model_ny_to_s, tokenizer=tokenizer_trans, device=0)\n",
    "def pipe_ny_to_s(prompts) :\n",
    "    return [out['generated_text'] for out in internal_pipe_ny_to_s(prompts)]\n",
    "def pipe_ny_to_s_tensor(prompts) :\n",
    "    inputs = [s.decode('utf-8') for s in prompts.numpy().tolist()]\n",
    "    return [out['generated_text'] for out in internal_pipe_ny_to_s(inputs)]\n",
    "\n",
    "# custom classification pipeline\n",
    "def pipe_disc(prompts, from_logits=False) :\n",
    "    inputs = tokenizer_disc(prompts, return_tensors='tf', padding=True, truncation=True)\n",
    "    logits = model_disc(**inputs).logits\n",
    "    if from_logits :\n",
    "        predicted_class_id = tf.math.reduce_max(logits, axis=-1)\n",
    "    else :\n",
    "        predicted_class_id = tf.math.argmax(logits, axis=-1) # 1 is shake, 0 is ny\n",
    "    return predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MadGKDDqFrlb"
   },
   "outputs": [],
   "source": [
    "## define function that will construct a mini dataset for the translators to train on\n",
    "def construct_parallel_corpus_tf(inputs, targets) :\n",
    "    model_inputs = tokenizer_trans(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    with tokenizer_trans.as_target_tokenizer() :\n",
    "        model_targets = tokenizer_trans(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "    ins = model_inputs['input_ids']\n",
    "    att = model_inputs['attention_mask']\n",
    "    lab = model_targets['input_ids']\n",
    "    lab = tf.where(lab == tokenizer_trans.pad_token_id, -100, lab)\n",
    "    return ins, att, lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ugKTueilFgvo"
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "@tf.custom_gradient\n",
    "def train_step(x_s_to_ny, x_ny_to_s, train_weight=1, adv_weight=1, verbose=False) :\n",
    "    with tf.GradientTape() as tape_s_to_ny, tf.GradientTape() as tape_ny_to_s :\n",
    "        tape_s_to_ny.watch(model_s_to_ny.trainable_variables)\n",
    "        tape_ny_to_s.watch(model_ny_to_s.trainable_variables)\n",
    "        # forward pass\n",
    "        # s to ny forward\n",
    "        if verbose : print(f\"Starting Shakespeare to New Yorker forward pass\")\n",
    "        if verbose : print(f\"    input size: {len(x_s_to_ny)} type: {type(x_s_to_ny)}\")\n",
    "        if verbose : print(f\"    input: {x_s_to_ny}\")\n",
    "        x_s_to_ny_out = pipe_s_to_ny(x_s_to_ny)\n",
    "        if verbose : print(f\"   output: {x_s_to_ny_out}\")\n",
    "        x_s_to_ny_out_disc = pipe_disc(x_s_to_ny_out, from_logits=True)\n",
    "        if verbose : print(f\"   discri: {x_s_to_ny_out_disc}\")\n",
    "        # ny to s forward\n",
    "        if verbose : print(f\"Starting New Yorker to Shakespeare forward pass\")\n",
    "        if verbose : print(f\"    input size: {len(x_ny_to_s)} type: {type(x_ny_to_s)}\")\n",
    "        if verbose : print(f\"    input: {x_ny_to_s}\")\n",
    "        x_ny_to_s_out = pipe_ny_to_s(x_ny_to_s)\n",
    "        if verbose : print(f\"   output: {x_ny_to_s_out}\")\n",
    "        x_ny_to_s_out_disc = pipe_disc(x_ny_to_s_out, from_logits=True)\n",
    "        if verbose : print(f\"   discri: {x_ny_to_s_out_disc}\")\n",
    "\n",
    "        # compute pseudo parallel corpora to train on\n",
    "        if verbose : print(f\"Constructing Parallel Corpus for Shakespeare to NY\")\n",
    "        inputs_ids_s_to_ny, attention_mask_s_to_ny, labels_s_to_ny = construct_parallel_corpus_tf(x_ny_to_s_out, x_ny_to_s)\n",
    "        if verbose : print(f\"Constructing Parallel Corpus for NY to Shakespeare\")\n",
    "        inputs_ids_ny_to_s, attention_mask_ny_to_s, labels_ny_to_s = construct_parallel_corpus_tf(x_s_to_ny_out, x_s_to_ny)\n",
    "\n",
    "        # train on pseudo parallel corpus data\n",
    "        if verbose : print(f\"Training S to NY model on pseudo parallel corpus\")\n",
    "        trainout_s_to_ny = model_s_to_ny(\n",
    "            input_ids = inputs_ids_s_to_ny,\n",
    "            attention_mask = attention_mask_s_to_ny,\n",
    "            labels = labels_s_to_ny,\n",
    "            training=True\n",
    "        )\n",
    "        if verbose : print(f\"Training NY to S model on pseudo parallel corpus\")\n",
    "        trainout_ny_to_s = model_ny_to_s(\n",
    "            input_ids = inputs_ids_ny_to_s,\n",
    "            attention_mask = attention_mask_ny_to_s,\n",
    "            labels = labels_ny_to_s,\n",
    "            training=True\n",
    "        )\n",
    "\n",
    "        # losses\n",
    "        if verbose : print(f\"Starting loss calculations for both models training phases\")\n",
    "        loss_s_to_ny_train = trainout_s_to_ny.loss\n",
    "        loss_ny_to_s_train = trainout_ny_to_s.loss\n",
    "        if verbose : print(f\"Starting additional discriminator loss calculations\")\n",
    "        loss_s_to_ny_adv = tf.keras.losses.binary_crossentropy([0 for i in range(len(x_s_to_ny))], x_s_to_ny_out_disc, from_logits=True)\n",
    "        loss_ny_to_s_adv = tf.keras.losses.binary_crossentropy([1 for i in range(len(x_ny_to_s))], x_ny_to_s_out_disc, from_logits=True)\n",
    "        # total losses\n",
    "        loss_s_to_ny = train_weight * loss_s_to_ny_train + adv_weight * loss_s_to_ny_adv\n",
    "        loss_ny_to_s = train_weight * loss_ny_to_s_train + adv_weight * loss_ny_to_s_adv\n",
    "        if verbose : print(f\"Calculated S to NY loss as {tf.round(loss_s_to_ny, 2)} (train loss = {tf.round(loss_s_to_ny_train, 2)}, adv loss = {tf.round(loss_s_to_ny_adv, 2)})\")\n",
    "        if verbose : print(f\"Calculated NY to S loss as {tf.round(loss_ny_to_s, 2)} (train loss = {tf.round(loss_ny_to_s_train, 2)}, adv loss = {tf.round(loss_ny_to_s_adv, 2)})\")\n",
    "\n",
    "\n",
    "\n",
    "    # gradients\n",
    "    if verbose : print(f\"Starting to apply gradients and update parameters for the translator models\")\n",
    "    gradients_s_to_ny = tape_s_to_ny.gradient(loss_s_to_ny, model_s_to_ny.trainable_variables)\n",
    "    optimizer_s_to_ny.apply_gradients(zip(gradients_s_to_ny, model_s_to_ny.trainable_variables))\n",
    "\n",
    "    gradients_ny_to_s = tape_ny_to_s.gradient(loss_ny_to_s, model_ny_to_s.trainable_variables)\n",
    "    optimizer_ny_to_s.apply_gradients(zip(gradients_ny_to_s, model_ny_to_s.trainable_variables))\n",
    "\n",
    "    # del loss_ny_to_s\n",
    "    # del loss_s_to_ny\n",
    "    # tf.keras.backend.clear_session()\n",
    "\n",
    "    if verbose : print(f\"Finished updating the translator models\")\n",
    "\n",
    "    return loss_s_to_ny, loss_ny_to_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZPShPNeFiah"
   },
   "source": [
    "### Now, train over some batches for a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lD_v4EUSFZD8"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "raw_x_s_to_ny = [training_data[i] for i in range(len(training_labels)) if labels[i] == 1]\n",
    "sny_cardinality = len(raw_x_s_to_ny)\n",
    "raw_x_s_to_ny = raw_x_s_to_ny + raw_x_s_to_ny[:batch_size]\n",
    "raw_x_ny_to_s = [training_data[i] for i in range(len(training_labels)) if labels[i] == 0]\n",
    "nys_cardinality = len(raw_x_ny_to_s)\n",
    "raw_x_ny_to_s = raw_x_ny_to_s + raw_x_ny_to_s[:batch_size]\n",
    "\n",
    "\n",
    "def get_training_batch(batch_num, batch_size=8) :\n",
    "    curr_sny_idx = (batch_num * batch_size) % sny_cardinality\n",
    "    curr_nys_idx = (batch_num * batch_size) % nys_cardinality\n",
    "    x_s_to_ny = raw_x_s_to_ny[curr_sny_idx:curr_sny_idx+batch_size]\n",
    "    x_ny_to_s = raw_x_ny_to_s[curr_nys_idx:curr_nys_idx+batch_size]\n",
    "    return x_s_to_ny, x_ny_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cDaUutVFc46",
    "outputId": "6ede2a49-fc43-468b-afd9-06318bea02d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n"
     ]
    }
   ],
   "source": [
    "num_batches_per_epoch = max(nys_cardinality//batch_size, sny_cardinality//batch_size)\n",
    "print(num_batches_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "O1S130xVFbjU",
    "outputId": "e44170f3-29e3-458b-b4da-fdcce4216db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting...\n",
      "Starting Shakespeare to New Yorker forward pass\n",
      "    input size: 2 type: <class 'list'>\n",
      "    input: [' 1 On a wall shadowed by lights from the distance is the screen.', 'This is America calling: The mirroring of state to state, Of voice to voice on the wires, The force of colloquial greetings like golden Pollen sinking on the afternoon breeze.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   output: ['1 On a wall shadowed by lights from the distance is the screen.', 'This is America calling: The mirroring of state to state, Of voice to voice on the']\n",
      "   discri: [4.5887227 3.5268695]\n",
      "Starting New Yorker to Shakespeare forward pass\n",
      "    input size: 2 type: <class 'list'>\n",
      "    input: ['We never feel the scratch, they do.', 'But I fancy I see, under the press of having to write the instruction manual, Your public square, city, with its elaborate little bandstand!']\n",
      "   output: ['We never feel the scratch, they do.', 'But I fancy I see, under the press of having to write the instruction manual, Your public']\n",
      "   discri: [4.2033362 4.232764 ]\n",
      "Constructing Parallel Corpus for Shakespeare to NY\n",
      "Constructing Parallel Corpus for NY to Shakespeare\n",
      "Training S to NY model on pseudo parallel corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NY to S model on pseudo parallel corpus\n",
      "Starting loss calculations for both models training phases\n",
      "Starting additional discriminator loss calculations\n",
      "Calculated S to NY loss as [7.] (train loss = [3.], adv loss = 4.0)\n",
      "Calculated NY to S loss as [3.] (train loss = [3.], adv loss = 0.0)\n",
      "Starting to apply gradients and update parameters for the translator models\n",
      "Finished updating the translator models\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         sig = _signature_from_callable(func,\n\u001b[0m\u001b[1;32m   1278\u001b[0m                                        \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2395\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2396\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is not a callable object'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.4577196], dtype=float32)> is not a callable object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-13ae246bc012>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx_s_to_ny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ny_to_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss_sny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_nys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_s_to_ny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ny_to_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madv_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mepoch_loss_sny\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_sny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mepoch_loss_nys\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_nys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;34m\"\"\"Decorated function with custom gradient.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_eager_mode_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_mode_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m_eager_mode_decorator\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m   ]\n\u001b[0;32m--> 552\u001b[0;31m   \u001b[0mgrad_argspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m   if (variables and (\"variables\" not in grad_argspec.args) and\n\u001b[1;32m    554\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0;34m\"variables\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_argspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwonlyargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_maybe_argspec_to_fullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_getfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;31m# else. So to be fully backwards compatible, we catch all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0;31m# possible exceptions here, and reraise a TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unsupported callable'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported callable"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "train_weight = 1\n",
    "adv_weight = 1\n",
    "##\n",
    "tf.config.optimizer.set_jit(True)\n",
    "for epoch in range(num_epochs) : # num epochs\n",
    "    print(f\"Epoch {epoch + 1} starting...\")\n",
    "    epoch_loss_sny = 0\n",
    "    epoch_loss_nys = 0\n",
    "    for batch_num in range(num_batches_per_epoch) :\n",
    "        x_s_to_ny, x_ny_to_s = get_training_batch(batch_num, batch_size=batch_size)\n",
    "        loss_sny, loss_nys = train_step(x_s_to_ny, x_ny_to_s, train_weight=train_weight, adv_weight=adv_weight, verbose=True)\n",
    "        epoch_loss_sny += loss_sny\n",
    "        epoch_loss_nys += loss_nys\n",
    "        print(f\"   Batch S to NY Loss: {loss_sny}\\n   Batch NY to S Loss: {loss_nys}\")\n",
    "    print(f\"Total Epoch S to NY Loss: {epoch_loss_sny}\\nTotal Epoch NY to S Loss: {epoch_loss_nys}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlSJx4Z4QJPN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
