{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1d829e-c8fc-4edf-9c9e-0540c2c30a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8c0ec2-342e-4c54-9a81-97642b2ef776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======\n",
      "\n",
      "\n",
      "Index(['poem name', 'content', 'author', 'type', 'age'], dtype='object') 13854\n",
      "Shakespeare: 85 examples\n",
      "New Yorkers: 81 examples\n",
      "Shakespeare avg length: 1468.5058823529412\n",
      "New Yorkers avg length: 1810.6049382716049\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/shahules786/PoetryFoundationData/data/train-00000-of-00001-486832872ed96d17.parquet\")\n",
    "print(f\"\\n\\n======\\n\\n\")\n",
    "\n",
    "print(df.columns, len(df))\n",
    "newyork = df[df['author'].isin([\"John Ashbery\", \"Barbara Guest\", \"James Schuyler\", \"Kenneth Koch\", \"Frank O'Hara\"])]\n",
    "shake = df[df['author'] == 'William Shakespeare']\n",
    "\n",
    "print(f\"Shakespeare: {len(shake)} examples\\nNew Yorkers: {len(newyork)} examples\")\n",
    "print(f\"Shakespeare avg length: {np.average([len(poem) for poem in shake['content']])}\\nNew Yorkers avg length: {np.average([len(poem) for poem in newyork['content']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecd7a4d-7185-4910-9292-e042a8859f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    }
   ],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each length 50\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-50\")\n",
    "    # wv_from_bin = api.load(\"glove.6B/glove.6B.50d.txt\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1031be-657a-4e09-86fb-5c592e3627f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3916, 277, 1130, 5168, 2838, 5188, 5137, 7516, 4134, 8039, 2896, 6310, 5526, 4076, 3548, 318, 5514, 4891, 1130, 2305, 3292, 56, 1549, 7151, 795, 2619, 351, 7546, 1442, 8442, 0, 6303, 386, 2551, 4242, 558, 7529, 3884, 2557, 8358, 3430, 3333, 2384, 7508, 8582, 4980, 4777, 7508, 8582, 238, 7546, 1374, 5137, 4277, 6393, 4695, 8442, 1754, 7516, 1740, 5520, 5220, 7255, 5516, 5107, 4092, 1267, 5713, 5107, 1268, 5073, 7542, 318, 1470, 7670, 4875, 8115, 5107, 224, 981, 7516, 4004, 3916, 2309, 7571, 4781, 351, 3692, 8202, 5188, 5107, 8589, 6249, 117, 867, 1663, 8442, 455, 238, 2932, 3135, 455, 3884, 4271, 2932, 3884, 7946, 4271, 8242, 3925, 5185, 2084, 238, 8242, 3925, 7516, 4561, 7516, 4423, 5107, 4260, 5163, 5107, 4331, 8286, 8325, 4593, 1581, 0, 4769, 6904, 7670, 541, 2910, 3766, 1660, 3122, 5107, 4271, 1535, 2127, 6559, 3766, 7571, 4781, 7510, 3409, 6485, 8049, 122, 3692, 4089, 7510, 3692, 844, 7695, 4833, 5137, 4875, 5238, 6850, 5399, 5107, 7561, 351, 7546, 1374, 7670, 4577, 7546, 318, 5728, 238, 205, 8395, 541, 8345, 7533, 318, 7516, 5728, 7855, 5107, 2464, 1911, 7670, 738, 4160, 4277, 3180, 1221, 6038, 5212, 0, 1266, 2987, 0, 3543, 7725, 7516, 5866, 7561, 7510, 3375, 7670, 4577, 238, 3692, 7471, 8582, 238, 8582, 4089, 3853, 8335, 3692, 4582, 8335, 6063, 5167, 5935, 993, 8409, 6214, 3537, 7526, 8345, 318, 7542, 6252, 3925, 3916, 7516, 4428, 238, 7783, 7510, 7471, 8049, 8356, 8286, 6574, 541, 4092, 238, 8356, 5185, 2632, 1032, 541, 2510, 4277, 0, 7039, 174, 7516, 6130, 3916, 8190, 2878, 0, 4238, 7510, 4944, 340, 1771, 117, 1771, 7516, 2484, 8011, 2732, 8582, 3430, 6200, 3925, 5152, 5014, 4090, 8335, 3925, 3916, 7516, 7891, 2408, 3350, 4446, 5137, 0, 5532, 7516, 4647, 8242, 8434, 238, 6476, 1942, 0, 4355, 7645, 137, 981, 3934, 7645, 3409, 7116, 5014, 339, 7472, 5107, 1745, 238, 7516, 4794, 4293, 7100, 7510, 1032, 541, 7405, 125, 1745, 5035, 238, 3766, 7516, 3028, 3766, 1535, 8554, 3766, 5862, 6824, 3641, 3766, 7516, 1583, 5185, 1583, 3766, 2690, 319, 3766, 1535, 6565, 7181]\n",
      "[3916, 277, 1130, 5168, 2838, 5188] 5137\n"
     ]
    }
   ],
   "source": [
    "def process_poem(poem) :\n",
    "  out = poem.replace('\\n', ' ')\n",
    "  out = out.lower()\n",
    "  out = re.sub(r'[^a-zA-Z ]+', '', out)\n",
    "  return [word for word in out.split(' ') if word!='']\n",
    "\n",
    "\n",
    "newyork_processed = [process_poem(newyork['content'].iloc[i]) for i in range(len(newyork))]\n",
    "newyork_labels = [0 for i in range(len(newyork))]\n",
    "shake_processed = [process_poem(shake['content'].iloc[i]) for i in range(len(shake))]\n",
    "shake_labels = [1 for i in range(len(shake))]\n",
    "# processed_poems = [process_poem(df['content'].iloc[i]) for i in range(len(df))]\n",
    "# print(processed_poems[0])\n",
    "# #newyork_labels = [0 for i in range(len(df))]\n",
    "\n",
    "processed_poems = newyork_processed + shake_processed\n",
    "# perm = np.random.permutation(len(processed_poems))\n",
    "# processed_poems = processed_poems[perm]\n",
    "# labels = labels[perm]\n",
    "\n",
    "vocab = sorted(list(set([word for poem in processed_poems for word in poem])))\n",
    "\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx:word for idx, word in enumerate(vocab)}\n",
    "input_sequences = []\n",
    "labels = []\n",
    "\n",
    "for poem in processed_poems :\n",
    "  for i in range(len(poem)) :\n",
    "    poem[i] = word_to_idx[poem[i]]\n",
    "    if i < len(poem) - 1:\n",
    "      n_gram_sequence = poem[max(0, i-99):i+1]\n",
    "      input_sequences.append(n_gram_sequence)\n",
    "    if i > 0:\n",
    "      labels.append(poem[i])\n",
    "\n",
    "print(processed_poems[0])\n",
    "print(input_sequences[5], labels[5])\n",
    "\n",
    "# print(processed_poems[0])\n",
    "\n",
    "# embedding_matrix = np.zeros((len(vocab), 50))\n",
    "# bad_count = 0\n",
    "# for i, word in enumerate(vocab):\n",
    "#     try:\n",
    "#         embedding_matrix[i] = wv_from_bin.get_vector(word)\n",
    "#     except:\n",
    "#         print(\"this is bad\", word)\n",
    "#         bad_count += 1\n",
    "# print(f\"Total Bad Words: {bad_count} out of total vocab {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92259646-dd57-4019-b319-613c9ca090b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 08:38:20.856360: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-16 08:38:22.351935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/lib\n",
      "2024-11-16 08:38:22.351961: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-16 08:38:22.517637: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-16 08:38:25.214908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/lib\n",
      "2024-11-16 08:38:25.215050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/lib\n",
      "2024-11-16 08:38:25.215062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-11-16 08:38:27.410813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/lib\n",
      "2024-11-16 08:38:27.412516: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-11-16 08:38:27.412585: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-0-136): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length = 100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = max(len(sequence) for sequence in input_sequences)\n",
    "print(f\"Max Length = {max_length}\")\n",
    "predictors = pad_sequences(input_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ff9ac12-14e6-4969-8bd8-1883cb382885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44818\n"
     ]
    }
   ],
   "source": [
    "print(len(predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371035d4-5909-4f9a-8fce-c43c644da1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 07:46:20.326448: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-16 07:46:21.691856: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/lib\n",
      "2024-11-16 07:46:21.691887: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-16 07:46:21.856517: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-16 07:46:24.402799: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/lib\n",
      "2024-11-16 07:46:24.402928: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/lib\n",
      "2024-11-16 07:46:24.402941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-11-16 07:46:27.639188: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/lib\n",
      "2024-11-16 07:46:27.641141: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-11-16 07:46:27.641170: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-0-136): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "# f = open('predictors.pickle', 'wb')\n",
    "# pickle.dump(predictors, f)\n",
    "# f.close()\n",
    "# f = open('labels.pickle', 'wb')\n",
    "# pickle.dump(labels, f)\n",
    "# f.close()\n",
    "\n",
    "f = open('predictors.pickle', 'rb')\n",
    "predictors = pickle.load(f)\n",
    "f.close()\n",
    "f = open('labels.pickle', 'rb')\n",
    "labels = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b38356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay\n",
      "nice\n",
      "cool\n"
     ]
    }
   ],
   "source": [
    "perm = np.random.permutation(len(predictors))\n",
    "print(\"yay\")\n",
    "shuffled_predictors = np.array(predictors)[perm]\n",
    "shuffled_labels = np.array(labels)[perm]\n",
    "print('nice')\n",
    "training_data = shuffled_predictors[:-5000]\n",
    "training_labels = shuffled_labels[:-5000]\n",
    "print('cool')\n",
    "validation_data = shuffled_predictors[-5000:]\n",
    "validation_labels = shuffled_labels[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc35f78",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7393db1a-470d-41cc-8db3-c437bb136103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 10)           86020     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8602)              868802    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 999,222\n",
      "Trainable params: 999,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "8602\n",
      "4399\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "import tensorflow.keras.utils as ku\n",
    "\n",
    "# len(vocab) = 161619\n",
    "model_lstm = tf.keras.Sequential()\n",
    "e = Embedding(len(vocab), 10, input_length = 100)  # used to be a trainable embedding layer to 10d\n",
    "model_lstm.add(e)\n",
    "model_lstm.add(LSTM(100))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(Dense(len(vocab), activation='sigmoid'))   # used to have len(vocab) neurons\n",
    "# lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(0.1, decay_rate=0.1, decay_steps=1000)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model_lstm.summary())\n",
    "print(len(vocab))\n",
    "print(training_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43a02a5-415d-42b1-bdd7-5db12108a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1245/1245 [==============================] - 98s 77ms/step - loss: 7.2963 - accuracy: 0.0488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5369c2fb20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(np.array(training_data), np.array(ku.to_categorical(training_labels, num_classes=len(vocab))), epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ec608f-e779-4a84-861f-9224bdfb21be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 4s 24ms/step - loss: 7.2250 - accuracy: 0.0538\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_lstm.evaluate(np.array(validation_data), np.array(ku.to_categorical(validation_labels, num_classes=len(vocab))), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7660ca1-7e6f-48dc-bd25-b5af1d0b80e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.225005626678467 0.05380000174045563\n"
     ]
    }
   ],
   "source": [
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b87cf-ecbe-498f-b171-0135d7140bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
