{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for the finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe columns: Index(['poem name', 'content', 'author', 'type', 'age'], dtype='object')\n",
      "Shakespeare: 85 examples\n",
      "New Yorkers: 81 examples\n",
      "Shakespeare avg length: 1468.5058823529412\n",
      "New Yorkers avg length: 1810.6049382716049\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/shahules786/PoetryFoundationData/data/train-00000-of-00001-486832872ed96d17.parquet\")\n",
    "\n",
    "print(f\"dataframe columns: {df.columns}\")\n",
    "\n",
    "newyork = df[df['author'].isin([\"John Ashbery\", \"Barbara Guest\", \"James Schuyler\", \"Kenneth Koch\", \"Frank O'Hara\"])]\n",
    "shake = df[df['author'] == 'William Shakespeare']\n",
    "\n",
    "print(f\"Shakespeare: {len(shake)} examples\\nNew Yorkers: {len(newyork)} examples\")\n",
    "print(f\"Shakespeare avg length: {np.average([len(poem) for poem in shake['content']])}\\nNew Yorkers avg length: {np.average([len(poem) for poem in newyork['content']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a sentence.',\n",
       " 'This is -another :SENTENCE!!!!!',\n",
       " 'AND this is a question?',\n",
       " 'again.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_poem(poem) :\n",
    "  proc = re.sub(r'[\\r\\n]+', ' ', poem)\n",
    "  proc = re.sub(r'\\s+', ' ', proc)\n",
    "  sentences = re.split(r'(?<=[.!?])\\s+', proc)\n",
    "  sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "  return sentences\n",
    "process_poem(\"this is a sentence. This is -another :SENTENCE!!!!!\\nAND this is a question? again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of New Yorker sentences: 1213 with avg length of 110.57378400659522 characters\n",
      "eg:\n",
      "    Is anything central?\n",
      "   Orchards flung out on the land, Urban forests, rustic plantations, knee-high hills?\n",
      "   Are place names central?\n",
      "   Elm Grove, Adcock Corner, Story Book Farm?\n",
      "   As they concur with a rush at eye level Beating themselves into eyes which have had enough Thank you, no more thank you.\n",
      "   And they come on like scenery mingled with darkness The damp plains, overgrown suburbs, Places of known civic pride, of civil obscurity.\n",
      "   These are connected to my version of America But the juice is elsewhere.\n",
      "   This morning as I walked out of your room After breakfast crosshatched with Backward and forward glances, backward into light, Forward into unfamiliar light, Was it our doing, and was it The material, the lumber of life, or of lives We were measuring, counting?\n",
      "   A mood soon to be forgotten In crossed girders of light, cool downtown shadow In this morning that has seized us again?\n",
      "   I know that I braid too much on my own Snapped-off perceptions of things as they come to me.\n",
      "\n",
      "Number of Shakespearean sentences: 645 with avg length of 174.8372093023256 characters\n",
      "eg:\n",
      "    Let the bird of loudest lay On the sole Arabian tree Herald sad and trumpet be, To whose sound chaste wings obey.\n",
      "   But thou shrieking harbinger, Foul precurrer of the fiend, Augur of the fever's end, To this troop come thou not near.\n",
      "   From this session interdict Every fowl of tyrant wing, Save the eagle, feather'd king; Keep the obsequy so strict.\n",
      "   Let the priest in surplice white, That defunctive music can, Be the death-divining swan, Lest the requiem lack his right.\n",
      "   And thou treble-dated crow, That thy sable gender mak'st With the breath thou giv'st and tak'st, 'Mongst our mourners shalt thou go.\n",
      "   Here the anthem doth commence: Love and constancy is dead; Phoenix and the Turtle fled In a mutual flame from hence.\n",
      "   So they lov'd, as love in twain Had the essence but in one; Two distincts, division none: Number there in love was slain.\n",
      "   Hearts remote, yet not asunder; Distance and no space was seen 'Twixt this Turtle and his queen: But in them it were a wonder.\n",
      "   So between them love did shine That the Turtle saw his right Flaming in the Phoenix' sight: Either was the other's mine.\n",
      "   Property was thus appalled That the self was not the same; Single nature's double name Neither two nor one was called.\n"
     ]
    }
   ],
   "source": [
    "newyork_processed = [] \n",
    "for i in range(len(newyork)) :\n",
    "   newyork_processed += process_poem(newyork['content'].iloc[i])\n",
    "newyork_labels = [0 for i in range(len(newyork_processed))]\n",
    "\n",
    "shake_processed = [] \n",
    "for i in range(len(shake)) :\n",
    "   shake_processed += process_poem(shake['content'].iloc[i])\n",
    "shake_labels = [1 for i in range(len(shake_processed))]\n",
    "\n",
    "processed_poems = newyork_processed + shake_processed\n",
    "labels = newyork_labels + shake_labels\n",
    "\n",
    "##\n",
    "\n",
    "print(f\"Number of New Yorker sentences: {len(newyork_processed)} with avg length of {np.mean([len(sentence) for sentence in newyork_processed])} characters\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {newyork_processed[i]}\")\n",
    "print(f\"\\nNumber of Shakespearean sentences: {len(shake_processed)} with avg length of {np.mean([len(sentence) for sentence in shake_processed])} characters\")\n",
    "print(f\"eg:\")\n",
    "for i in range(10) :\n",
    "   print(f\"   {shake_processed[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length = 2707\n",
      "Avg Length = 132.8826695371367\n"
     ]
    }
   ],
   "source": [
    "sentence_lengths = [len(poem) for poem in processed_poems]\n",
    "max_length = max(sentence_lengths)\n",
    "avg_length = np.mean(sentence_lengths)\n",
    "print(f\"Max Length = {max_length}\\nAvg Length = {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(processed_poems))\n",
    "shuffled_poems = np.array(processed_poems)[perm]\n",
    "shuffled_labels = np.array(labels)[perm]\n",
    "\n",
    "training_data = shuffled_poems[:-100]\n",
    "training_labels = shuffled_labels[:-100]\n",
    "\n",
    "validation_data = shuffled_poems[-100:]\n",
    "validation_labels = shuffled_labels[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_dict({'text': training_data, 'label': training_labels})\n",
    "validation_ds = Dataset.from_dict({'text': validation_data, 'label': validation_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load discriminator model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Flatten\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM, TFAutoModelForSequenceClassification, TFT5ForConditionalGeneration, pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anthony\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2ForSequenceClassification.\n",
      "\n",
      "All the weights of TFGPT2ForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2ForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load discriminator model\n",
    "model_disc = TFAutoModelForSequenceClassification.from_pretrained('gpt2_discriminator')\n",
    "model_disc.config.pad_token_id = model_disc.config.eos_token_id\n",
    "tokenizer_disc = AutoTokenizer.from_pretrained('gpt2_discriminator_tokenizer')\n",
    "tokenizer_disc.pad_token = tokenizer_disc.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom classification pipeline\n",
    "def pipe_disc(prompts, from_logits=False) :\n",
    "    inputs = tokenizer_disc(prompts, return_tensors='tf', padding=True, truncation=True)\n",
    "    logits = model_disc(**inputs).logits\n",
    "    if from_logits :\n",
    "        predicted_class_id = tf.math.reduce_max(logits, axis=-1)\n",
    "    else :\n",
    "        predicted_class_id = tf.math.argmax(logits, axis=-1) # 1 is shake, 0 is ny\n",
    "    return predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = validation_data.tolist()\n",
    "model_pred = pipe_disc(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 correct out of 100 total\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(validation_labels)) :\n",
    "    total += 1\n",
    "    correct += 1 if validation_labels[i] == model_pred[i] else 0\n",
    "print(f\"{correct} correct out of {total} total\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
